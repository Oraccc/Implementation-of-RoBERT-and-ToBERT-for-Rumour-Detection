{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ec3077e",
   "metadata": {},
   "source": [
    "### Step 1.1 : Data Viewing and Simple Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f01a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers.data.processors.utils import InputExample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc778bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = pd.read_csv('./data/raw_data.csv')\n",
    "raw_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f731331f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These codes are used for data statistics only. No need to uncomment.\n",
    "\n",
    "# print(raw_data['count'].median())\n",
    "\n",
    "# raw_data['len_text'] =raw_data.text_comments.apply(lambda x: len(x.split()))\n",
    "\n",
    "# print(raw_data['len_text'].median())\n",
    "# bins = [0,50,100,150,200,250,300,350,400,450,500]\n",
    "# groups = pd.cut(raw_data['len_text'],bins,right=True)\n",
    "# pd.value_counts(groups).sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac565718",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "raw_data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0358b135",
   "metadata": {},
   "outputs": [],
   "source": [
    "### IMPORTANT ###\n",
    "# You may change 'model_path' to save and load different trained models.\n",
    "# Availiable options: 'text_comments','text_only','commments_only','comments_group1','comments_group2','comments_group3','natural_split'.\n",
    "# Please make sure that your 'model_path' must match the correspongding data and comments.\n",
    "# For more details, please check the 'README.md' file.\n",
    "\n",
    "model_path = 'text_comments'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc5ff3cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Different Number of Comments ##\n",
    "\n",
    "# Please uncomment the corresponding lines if the 'model_path' is 'comments_groupX'.\n",
    "\n",
    "# print(raw_data['count'].describe(percentiles=[0.33,0.67]))\n",
    "\n",
    "# For'comments_group1'.\n",
    "# raw_data = raw_data[raw_data['count'] <= 7]\n",
    "# raw_data.shape\n",
    "\n",
    "# For'comments_group2'.\n",
    "# raw_data = raw_data[raw_data['count'] > 7]\n",
    "# raw_data = raw_data[raw_data['count'] <= 18]\n",
    "# raw_data.shape\n",
    "\n",
    "# For'comments_group3'.\n",
    "# raw_data = raw_data[raw_data['count'] > 18]\n",
    "# raw_data.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48190230",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Data Selection ##\n",
    "\n",
    "# You may change 'text_comments' to 'text_only' or 'comments_only' with the corresponding 'model_path' to get more experiment results.\n",
    "\n",
    "raw_data = raw_data[['text_comments','label']]\n",
    "raw_data = raw_data.rename(columns = {'text_comments':'text'})\n",
    "\n",
    "# raw_data = raw_data[['text_only','label']]\n",
    "# raw_data = raw_data.rename(columns = {'text_only':'text'})\n",
    "\n",
    "# raw_data = raw_data[['comments_only','label']]\n",
    "# raw_data = raw_data.rename(columns = {'comments_only':'text'})\n",
    "\n",
    "raw_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c168aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = raw_data.dropna(axis=0)\n",
    "raw_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c43adc08",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data['label'] = LabelEncoder().fit_transform(raw_data['label'])\n",
    "raw_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e1f495",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = raw_data.copy()\n",
    "data = data.reindex(np.random.permutation(data.index))\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f15a36ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, val = train_test_split(data, test_size=0.2, random_state=35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d317016d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.reset_index(drop=True, inplace=True)\n",
    "val.reset_index(drop=True, inplace=True)\n",
    "\n",
    "train.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6155b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.shape,val.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efc3a980",
   "metadata": {},
   "source": [
    "### Step 1.2 : Split the Dataset into Segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f8ff90",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import get_split,get_natural_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7880649f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tmp = train.copy()\n",
    "\n",
    "### IMPORTANT ###\n",
    "# If your 'model_path' is 'natural_split' please use 'get_natural_split' function rather than 'get_split'.\n",
    "\n",
    "train_tmp['text_split'] = train['text'].apply(get_split)\n",
    "# train_tmp['text_split'] = train['text'].apply(get_natural_split)\n",
    "train = train_tmp\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc58a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_tmp = val.copy()\n",
    "\n",
    "### IMPORTANT ###\n",
    "# If your 'model_path' is 'natural_split' please use 'get_natural_split' function rather than 'get_split'.\n",
    "\n",
    "val_tmp['text_split'] = val['text'].apply(get_split)\n",
    "# val_tmp['text_split'] = val['text'].apply(get_natural_split)\n",
    "val = val_tmp\n",
    "val.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "461b2042",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_l = []  # Segmented Text\n",
    "label_l = []  # Label of Each Text\n",
    "index_l =[]   # The Index of Each Text Before Segmentation\n",
    "for idx,row in train.iterrows():\n",
    "  for l in row['text_split']:\n",
    "    train_l.append(l)\n",
    "    label_l.append(row['label'])\n",
    "    index_l.append(idx)\n",
    "len(train_l), len(label_l), len(index_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca1239b",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_l = []\n",
    "val_label_l = []\n",
    "val_index_l = []\n",
    "for idx,row in val.iterrows():\n",
    "  for l in row['text_split']:\n",
    "    val_l.append(l)\n",
    "    val_label_l.append(row['label'])\n",
    "    val_index_l.append(idx)\n",
    "len(val_l), len(val_label_l), len(val_index_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e06d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.DataFrame({'text':train_l, 'label':label_l})\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e92fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_df = pd.DataFrame({'text':val_l, 'label':val_label_l})\n",
    "val_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b9eb037",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_InputExamples = train_df.apply(lambda x: InputExample(guid=None,text_a = x['text'], text_b = None, label = x['label']), axis = 1)\n",
    "\n",
    "val_InputExamples = val_df.apply(lambda x: InputExample(guid=None, text_a = x['text'], text_b = None, label = x['label']), axis = 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c94ce1",
   "metadata": {},
   "source": [
    "### Step 2 : Define Models For Bert Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fdadd57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import CrossEntropyLoss, MSELoss\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch.optim as optim\n",
    "from torchtext.data import BucketIterator\n",
    "\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler, TensorDataset\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "from transformers import (\n",
    "    WEIGHTS_NAME,\n",
    "    AdamW,\n",
    "    get_linear_schedule_with_warmup,\n",
    "    BertConfig,\n",
    "    BertModel,\n",
    "    BertPreTrainedModel,\n",
    "    BertTokenizer,\n",
    "    BertweetTokenizer,\n",
    "    AutoModel,\n",
    "    AutoTokenizer\n",
    ")\n",
    "\n",
    "from transformers import glue_convert_examples_to_features as convert_examples_to_features\n",
    "from transformers import glue_output_modes as output_modes\n",
    "from transformers import glue_processors as processors\n",
    "from transformers.data.processors.utils import InputExample, DataProcessor\n",
    "\n",
    "import logging\n",
    "\n",
    "logger=logging.getLogger(__name__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f5144d",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_CLASSES={\n",
    "    \"bert\":(BertConfig,BertTokenizer),\n",
    "    \"bertweet\":(BertConfig,BertweetTokenizer)\n",
    "}\n",
    "\n",
    "my_label_list=[0, 1]\n",
    "MAX_SEQ_LENGTH=200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70daba27",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertForClassification(BertPreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.num_labels = 2\n",
    "\n",
    "        self.bert = BertModel(config)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.classifier = nn.Linear(config.hidden_size, self.num_labels)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        token_type_ids=None,\n",
    "        position_ids=None,\n",
    "        head_mask=None,\n",
    "        inputs_embeds=None,\n",
    "        labels=None,\n",
    "    ):\n",
    "\n",
    "        outputs = self.bert(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "        )\n",
    "\n",
    "        sequence_output, pooled_output=outputs[:2]\n",
    "\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.classifier(pooled_output)\n",
    "\n",
    "        \n",
    "        outputs = (logits, pooled_output, sequence_output,)\n",
    "\n",
    "        if labels is not None:\n",
    "            \n",
    "            if self.num_labels == 1:\n",
    "                loss_fct = MSELoss()\n",
    "                loss = loss_fct(logits.view(-1), labels.view(-1))\n",
    "            else:\n",
    "                loss_fct = CrossEntropyLoss()\n",
    "                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "            outputs = (loss,) + outputs\n",
    "\n",
    "        \n",
    "        return outputs  # loss, logits, pooled_output, sequence_output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b72192",
   "metadata": {},
   "source": [
    "### Step 3.1 : Load Pre-training Models & Prepare Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb4e239b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load Pre-training Models\n",
    "# args={\"model_name_or_path\": \"bert-base-uncased\",\n",
    "#     \"config_name\": \"bert-base-uncased\",\n",
    "#     \"tokenizer_name\": \"bert-base-uncased\",\n",
    "#       }\n",
    "\n",
    "# config_class, tokenizer_class = MODEL_CLASSES[\"bert\"]\n",
    "# model_class=BertForClassification\n",
    "\n",
    "\n",
    "# config = config_class.from_pretrained(\n",
    "#     args[\"config_name\"],\n",
    "#     finetuning_task=\"\", \n",
    "#     cache_dir=None,\n",
    "# )\n",
    "# tokenizer = tokenizer_class.from_pretrained(\n",
    "#     args[\"tokenizer_name\"],\n",
    "#     do_lower_case=True,\n",
    "#     cache_dir=None,\n",
    "# )\n",
    "# model = model_class.from_pretrained(\n",
    "#     args[\"model_name_or_path\"],\n",
    "#     from_tf=bool(\".ckpt\" in args[\"model_name_or_path\"]),\n",
    "#     config=config,\n",
    "#     cache_dir=None,\n",
    "# )\n",
    "\n",
    "\n",
    "# model.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b19bfa10",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# # Prepare Training Data\n",
    "# train_features = convert_examples_to_features(train_InputExamples,tokenizer, label_list=my_label_list, \n",
    "#                                               output_mode=\"classification\", max_length=MAX_SEQ_LENGTH )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081134e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_ids = torch.tensor([f.input_ids for f in train_features], dtype=torch.long)\n",
    "# attention_mask = torch.tensor([f.attention_mask for f in train_features], dtype=torch.long)\n",
    "# token_type_ids = torch.tensor([f.token_type_ids for f in train_features], dtype=torch.long)\n",
    "# the_labels = torch.tensor([f.label for f in train_features], dtype=torch.long)\n",
    "\n",
    "\n",
    "# dataset = TensorDataset(input_ids, attention_mask, token_type_ids, the_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89481e18",
   "metadata": {},
   "source": [
    "### Step 3.2 : Train & Save Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d094b419",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define Train Function For Bert Classification\n",
    "\n",
    "# def train(train_dataset,model,tokenizer):\n",
    "#     no_decay=[\"bias\",\"LayerNorm.weight\"]\n",
    "#     optimizer_grouped_parameters=[\n",
    "#         {\n",
    "#             \"params\":[p for n,p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "#             \"weight_decay\":0.0,\n",
    "\n",
    "#         },\n",
    "#         {\n",
    "#             \"params\": [p for n,p in model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "#             \"weight_decay\":0.0\n",
    "#         },\n",
    "#     ]\n",
    "\n",
    "    \n",
    "#     t_total=len(train_dataset)// 5\n",
    "#     optimizer=AdamW(optimizer_grouped_parameters,lr=2e-5,eps=1e-8)\n",
    "    \n",
    "#     scheduler=get_linear_schedule_with_warmup(\n",
    "#         optimizer,num_warmup_steps=0,num_training_steps=t_total\n",
    "#         )\n",
    "    \n",
    "    \n",
    "    \n",
    "#     # *********************\n",
    "#     logger.info(\"*****Running training*****\")\n",
    "#     logger.info(\"  Num examples = %d\", len(train_dataset))\n",
    "#     logger.info(\"  Num Epochs = %d\", 5)\n",
    "\n",
    "\n",
    "#     epochs_trained=0\n",
    "#     global_step=0\n",
    "#     steps_trained_in_current_epoch=0\n",
    "\n",
    "#     tr_loss,logging_loss=0.0,0.0\n",
    "#     model.zero_grad()\n",
    "#     train_iterator=trange(epochs_trained,5,desc=\"Epoch\",disable=False)\n",
    "\n",
    "\n",
    "#     for k in train_iterator: #5 epoch\n",
    "    \n",
    "#         train_sampler=RandomSampler(train_dataset)\n",
    "#         train_dataloader=DataLoader(train_dataset,sampler=train_sampler,batch_size=16)\n",
    "#         epoch_iterator=tqdm(train_dataloader,desc=\"Iteration\",disable=False)\n",
    "\n",
    "#         for step,batch in enumerate(epoch_iterator): \n",
    "#             if steps_trained_in_current_epoch>0:\n",
    "#                 steps_traned_in_current_epoch-=1\n",
    "#                 continue\n",
    "\n",
    "#             model.train()\n",
    "#             batch=tuple(t.to(\"cuda\") for t in batch)\n",
    "            \n",
    "#             inputs={\"input_ids\": batch[0],\"attention_mask\": batch[1],\"token_type_ids\": batch[2], \"labels\": batch[3]}\n",
    "   \n",
    "#             outputs = model(**inputs)\n",
    "#             loss=outputs[0]\n",
    " \n",
    "#             loss.backward()\n",
    "\n",
    "#             tr_loss+=loss.item()\n",
    "#             if (step+1)%1==0:\n",
    "#                 torch.nn.utils.clip_grad_norm_(model.parameters(),1.0)\n",
    "\n",
    "#                 optimizer.step()\n",
    "#                 scheduler.step()\n",
    "#                 model.zero_grad()\n",
    "#                 global_step+=1\n",
    "\n",
    "#         logger.info(\"average loss:\" +str(tr_loss/global_step))\n",
    "\n",
    "\n",
    "#     return global_step,tr_loss/global_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be86380b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Start Training\n",
    "\n",
    "# torch.cuda.empty_cache()\n",
    "# train(dataset,model,tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ed6847",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save Trained Model Parameters\n",
    "\n",
    "# import os\n",
    "# model.save_pretrained(\"./trained_models/classification_models_\" + model_path)\n",
    "# tokenizer.save_pretrained(\"./trained_models/classification_models_\" + model_path)\n",
    "\n",
    "# torch.save(args,os.path.join(\"./trained_models/classification_models_\" + model_path,\"training_args.bin\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b3204b4",
   "metadata": {},
   "source": [
    "### Step 4.1 : Load the Trained Model & Prepare Data for Bert Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10539792",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start Loading the trained model data\n",
    "\n",
    "args_eval={\"model_name_or_path\": \"./trained_models/classification_models_\" + model_path,\n",
    "    \"config_name\": \"./trained_models/classification_models_\" + model_path,\n",
    "    \"tokenizer_name\": \"./trained_models/classification_models_\" + model_path,\n",
    "      }\n",
    "\n",
    "config_class, tokenizer_class = MODEL_CLASSES[\"bert\"]\n",
    "model_class=BertForClassification\n",
    "\n",
    "\n",
    "config = config_class.from_pretrained(\n",
    "    args_eval[\"config_name\"],\n",
    "    finetuning_task=\"\", \n",
    "    cache_dir=None,\n",
    ")\n",
    "tokenizer = tokenizer_class.from_pretrained(\n",
    "    args_eval[\"tokenizer_name\"],\n",
    "    do_lower_case=True,\n",
    "    cache_dir=None,\n",
    ")\n",
    "model = model_class.from_pretrained(\n",
    "    args_eval[\"model_name_or_path\"],\n",
    "    from_tf=bool(\".ckpt\" in args_eval[\"model_name_or_path\"]),\n",
    "    config=config,\n",
    "    cache_dir=None,\n",
    ")\n",
    "\n",
    "\n",
    "model.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a5770d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare Data for Evaluation\n",
    "\n",
    "val_features = convert_examples_to_features(val_InputExamples, tokenizer, label_list=my_label_list, output_mode=\"classification\",  max_length=MAX_SEQ_LENGTH )\n",
    "\n",
    "\n",
    "val_input_ids = torch.tensor([f.input_ids for f in val_features], dtype=torch.long)\n",
    "val_attention_mask = torch.tensor([f.attention_mask for f in val_features], dtype=torch.long)\n",
    "val_token_type_ids = torch.tensor([f.token_type_ids for f in val_features], dtype=torch.long)\n",
    "val_the_labels = torch.tensor([f.label for f in val_features], dtype=torch.long)\n",
    "\n",
    "\n",
    "eval_dataset = TensorDataset(val_input_ids, val_attention_mask, val_token_type_ids, val_the_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d27663f",
   "metadata": {},
   "source": [
    "### Step 4.2 : Bert Classification Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e422409",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a5ba125",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, tokenizer, eval_dataset):\n",
    "\n",
    "\n",
    "    logger.info(\"***** Running evaluation  *****\")\n",
    "    logger.info(\"  Num examples = %d\", len(eval_dataset))\n",
    "    logger.info(\"  Batch size = %d\", 16)\n",
    "    eval_loss = 0.0\n",
    "    nb_eval_steps = 0\n",
    "    preds = None\n",
    "    out_label_ids = None\n",
    "\n",
    "    eval_sampler =RandomSampler(eval_dataset)\n",
    "    eval_dataloader = DataLoader(eval_dataset, sampler=eval_sampler, batch_size=16)\n",
    "\n",
    "    for batch in tqdm(eval_dataloader, desc=\"Evaluating\"):\n",
    "        model.eval()\n",
    "        batch = tuple(t.to(\"cuda\") for t in batch)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            inputs = {\"input_ids\": batch[0], \"attention_mask\": batch[1], \"labels\": batch[3]}\n",
    "            outputs = model(**inputs)\n",
    "            tmp_eval_loss, logits = outputs[:2]\n",
    "\n",
    "            eval_loss += tmp_eval_loss.mean().item()\n",
    "        nb_eval_steps += 1\n",
    "        if preds is None:\n",
    "            preds = logits.detach().cpu().numpy()\n",
    "            out_label_ids = inputs[\"labels\"].detach().cpu().numpy()\n",
    "        else:\n",
    "            preds = np.append(preds, logits.detach().cpu().numpy(), axis=0)\n",
    "            out_label_ids = np.append(out_label_ids, inputs[\"labels\"].detach().cpu().numpy(), axis=0)\n",
    "\n",
    "    eval_loss = eval_loss / nb_eval_steps\n",
    "\n",
    "    preds = np.argmax(preds, axis=1)\n",
    "    \n",
    "    accuracy,f1 = acc_and_f1(preds, out_label_ids)\n",
    "\n",
    "\n",
    "    return accuracy,f1,eval_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b2f0e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_accuracy(preds, labels):\n",
    "    return (preds == labels).mean()\n",
    "\n",
    "def acc_and_f1(preds, labels):\n",
    "    acc = simple_accuracy(preds, labels)\n",
    "    f1 = f1_score(y_true=labels, y_pred=preds)\n",
    "    return acc, f1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e7256e",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy,f1 ,eval_loss = evaluate(model, tokenizer, eval_dataset)\n",
    "\n",
    "print(\"Accuracy: \",accuracy, \"F1 Score: \",f1,\"Loss: \",eval_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "168cc642",
   "metadata": {},
   "source": [
    "### Step 5.1 : Get Text Embeddings & Combine Embeddings with Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "309a5709",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prediction(model, tokenizer, dataset):\n",
    "\n",
    "    logger.info(\"***** Running prediction  *****\")\n",
    "    logger.info(\"  Num examples = %d\", len(dataset))\n",
    "    logger.info(\"  Batch size = %d\", 16)\n",
    "\n",
    "    pooled_outputs = None\n",
    "\n",
    "    sampler =SequentialSampler(dataset)\n",
    "    dataloader = DataLoader(dataset, sampler=sampler, batch_size=32)\n",
    "\n",
    "    for batch in tqdm(dataloader, desc=\"Evaluating\"):\n",
    "        model.eval()\n",
    "        batch = tuple(t.to(\"cuda\") for t in batch)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            inputs = {\"input_ids\": batch[0], \"attention_mask\": batch[1], \"labels\": batch[3]}\n",
    "            outputs = model(**inputs)\n",
    "            pooled_output = outputs[2]\n",
    "\n",
    "            if pooled_outputs is None:\n",
    "                pooled_outputs = pooled_output.detach().cpu().numpy()\n",
    "            else:\n",
    "                pooled_outputs = np.append(pooled_outputs, pooled_output.detach().cpu().numpy(), axis=0)\n",
    "\n",
    "    return pooled_outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb0f8389",
   "metadata": {},
   "outputs": [],
   "source": [
    "args_eval={\"model_name_or_path\": \"./trained_models/classification_models_\" + model_path,\n",
    "    \"config_name\": \"./trained_models/classification_models_\" + model_path,\n",
    "    \"tokenizer_name\": \"./trained_models/classification_models_\" + model_path,\n",
    "      }\n",
    "\n",
    "\n",
    "config_class, tokenizer_class = MODEL_CLASSES[\"bert\"]\n",
    "model_class=BertForClassification\n",
    "\n",
    "\n",
    "config = config_class.from_pretrained(\n",
    "    args_eval[\"config_name\"],\n",
    "    finetuning_task=\"\", \n",
    "    cache_dir=None,\n",
    ")\n",
    "tokenizer = tokenizer_class.from_pretrained(\n",
    "    args_eval[\"tokenizer_name\"],\n",
    "    do_lower_case=True,\n",
    "    cache_dir=None,\n",
    ")\n",
    "model = model_class.from_pretrained(\n",
    "    args_eval[\"model_name_or_path\"],\n",
    "    from_tf=bool(\".ckpt\" in args_eval[\"model_name_or_path\"]),\n",
    "    config=config,\n",
    "    cache_dir=None,\n",
    ")\n",
    "\n",
    "\n",
    "model.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b830b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = convert_examples_to_features(train_InputExamples,tokenizer, label_list=my_label_list, output_mode=\"classification\", max_length=MAX_SEQ_LENGTH )\n",
    "\n",
    "val_features = convert_examples_to_features(val_InputExamples, tokenizer, label_list=my_label_list, output_mode=\"classification\",  max_length=MAX_SEQ_LENGTH )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "999a86a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input_ids = torch.tensor([f.input_ids for f in train_features], dtype=torch.long)\n",
    "train_attention_mask = torch.tensor([f.attention_mask for f in train_features], dtype=torch.long)\n",
    "train_token_type_ids = torch.tensor([f.token_type_ids for f in train_features], dtype=torch.long)\n",
    "train_the_labels = torch.tensor([f.label for f in train_features], dtype=torch.long)\n",
    "\n",
    "train_dataset = TensorDataset(train_input_ids, train_attention_mask, train_token_type_ids, train_the_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e5ee70",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_input_ids = torch.tensor([f.input_ids for f in val_features], dtype=torch.long)\n",
    "val_attention_mask = torch.tensor([f.attention_mask for f in val_features], dtype=torch.long)\n",
    "val_token_type_ids = torch.tensor([f.token_type_ids for f in val_features], dtype=torch.long)\n",
    "val_the_labels = torch.tensor([f.label for f in val_features], dtype=torch.long)\n",
    "\n",
    "val_dataset = TensorDataset(val_input_ids, val_attention_mask, val_token_type_ids, val_the_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297a56c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pooled_outputs = get_prediction(model, tokenizer, train_dataset)\n",
    "train_pooled_outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d3df64",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_pooled_outputs = get_prediction(model, tokenizer, val_dataset)\n",
    "val_pooled_outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a729bc29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 1: Feature Concatenation\n",
    "train_x = {}\n",
    "for l, emb in zip(index_l, train_pooled_outputs):\n",
    "    if l in train_x.keys():\n",
    "        # np.vstack on lists represents features concatenation \n",
    "        train_x[l]  =np.vstack([train_x[l], emb])\n",
    "    else:\n",
    "        train_x[l] = [emb]\n",
    "\n",
    "train_l_final = []\n",
    "label_l_final = []\n",
    "for k in train_x.keys():\n",
    "    train_l_final.append(train_x[k])\n",
    "    label_l_final.append(train.loc[k]['label'])\n",
    "\n",
    "df_train = pd.DataFrame({'emb': train_l_final, 'label': label_l_final})\n",
    "df_train.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c931f6b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 1: Feature Concatenation\n",
    "val_x = {}\n",
    "\n",
    "for l, emb in zip(val_index_l, val_pooled_outputs):\n",
    "    if l in val_x.keys():\n",
    "        val_x[l]  =np.vstack([val_x[l], emb])\n",
    "    else:\n",
    "        val_x[l] = [emb]\n",
    "\n",
    "\n",
    "val_l_final = []\n",
    "vlabel_l_final = []\n",
    "for k in val_x.keys():\n",
    "    val_l_final.append(val_x[k])\n",
    "    vlabel_l_final.append(val.loc[k]['label'])\n",
    "\n",
    "df_val = pd.DataFrame({'emb': val_l_final, 'label': vlabel_l_final})\n",
    "df_val.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a84d0d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Method 2: Feature Average Pooling\n",
    "# train_x = {}\n",
    "# for l, emb in zip(index_l, train_pooled_outputs):\n",
    "#     if l in train_x.keys():\n",
    "#         train_x[l]  =np.vstack([train_x[l], emb])\n",
    "#     else:\n",
    "#         train_x[l] = [emb]\n",
    "\n",
    "# for l in train_x.keys():\n",
    "#     # print(len(train_x[l]))\n",
    "#     train_x[l] = [np.mean(train_x[l],axis=0)]\n",
    "\n",
    "# train_l_final = []\n",
    "# label_l_final = []\n",
    "# for k in train_x.keys():\n",
    "#     train_l_final.append(train_x[k])\n",
    "#     label_l_final.append(train.loc[k]['label'])\n",
    "\n",
    "# df_train = pd.DataFrame({'emb': train_l_final, 'label': label_l_final})\n",
    "# df_train.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee847f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Method 2: Feature Average Pooling\n",
    "# val_x = {}\n",
    "\n",
    "# for l, emb in zip(val_index_l, val_pooled_outputs):\n",
    "#     if l in val_x.keys():\n",
    "#         val_x[l]  =np.vstack([val_x[l], emb])\n",
    "#     else:\n",
    "#         val_x[l] = [emb]\n",
    "\n",
    "# for l in val_x.keys():\n",
    "#     val_x[l] = [np.mean(val_x[l],axis=0)]\n",
    "\n",
    "# val_l_final = []\n",
    "# vlabel_l_final = []\n",
    "# for k in val_x.keys():\n",
    "#     val_l_final.append(val_x[k])\n",
    "#     vlabel_l_final.append(val.loc[k]['label'])\n",
    "\n",
    "# df_val = pd.DataFrame({'emb': val_l_final, 'label': vlabel_l_final})\n",
    "# df_val.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ad73f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Method 3: Feature Max Pooling\n",
    "# train_x = {}\n",
    "# for l, emb in zip(index_l, train_pooled_outputs):\n",
    "#     if l in train_x.keys():\n",
    "#         train_x[l]  =np.vstack([train_x[l], emb])\n",
    "#     else:\n",
    "#         train_x[l] = [emb]\n",
    "\n",
    "# for l in train_x.keys():\n",
    "#     # print(len(train_x[l]))\n",
    "#     train_x[l] = [np.max(train_x[l],axis=0)]\n",
    "\n",
    "# train_l_final = []\n",
    "# label_l_final = []\n",
    "# for k in train_x.keys():\n",
    "#     train_l_final.append(train_x[k])\n",
    "#     label_l_final.append(train.loc[k]['label'])\n",
    "\n",
    "# df_train = pd.DataFrame({'emb': train_l_final, 'label': label_l_final})\n",
    "# df_train.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f8fa895",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Method 3: Feature Max Pooling\n",
    "# val_x = {}\n",
    "\n",
    "# for l, emb in zip(val_index_l, val_pooled_outputs):\n",
    "#     if l in val_x.keys():\n",
    "#         val_x[l]  =np.vstack([val_x[l], emb])\n",
    "#     else:\n",
    "#         val_x[l] = [emb]\n",
    "\n",
    "# for l in val_x.keys():\n",
    "#     val_x[l] = [np.max(val_x[l],axis=0)]\n",
    "\n",
    "# val_l_final = []\n",
    "# vlabel_l_final = []\n",
    "# for k in val_x.keys():\n",
    "#     val_l_final.append(val_x[k])\n",
    "#     vlabel_l_final.append(val.loc[k]['label'])\n",
    "\n",
    "# df_val = pd.DataFrame({'emb': val_l_final, 'label': vlabel_l_final})\n",
    "# df_val.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "882b784c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_val, df_test = train_test_split(df_val, test_size=0.4, random_state=35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4faadf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.shape, df_val.shape, df_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96ff7d01",
   "metadata": {},
   "source": [
    "### Step 5.2 : Prepare Data for Classfication Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "204ebd06",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_dict = {\n",
    "    'text_comments':[[7,663],[3,232],[5,93]],\n",
    "    'text_only':[[7,663],[3,232],[5,93]],\n",
    "    'comments_only':[[4,1088],[4,163],[4,109]],\n",
    "    'comments_group1':[[4,387],[4,58],[5,31]],\n",
    "    'comments_group2':[[4,398],[1,239],[4,40]],\n",
    "    'comments_group3':[[5,300],[5,45],[1,151]],\n",
    "    'natural_split':[[7,663],[3,232],[5,93]],\n",
    "}\n",
    "\n",
    "batches = batch_dict[model_path]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a8941fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_generator(df, batch_size = batches[0][0], batches_per_epoch = batches[0][1]):\n",
    "    num_sequences = len(df['emb'].to_list())\n",
    "    assert batch_size * batches_per_epoch == num_sequences\n",
    "    num_features= 768\n",
    "\n",
    "    x_list= df['emb'].to_list()\n",
    "    y_list =  df.label.to_list()\n",
    "    # Generate batches\n",
    "    while True:\n",
    "        for b in range(batches_per_epoch):\n",
    "            longest_index = (b + 1) * batch_size - 1\n",
    "            timesteps = len(max(df['emb'].to_list()[:(b + 1) * batch_size][-batch_size:], key=len))\n",
    "            x_train = np.full((batch_size, timesteps, num_features), -99.)\n",
    "            y_train = np.zeros((batch_size,  1))\n",
    "            for i in range(batch_size):\n",
    "                li = b * batch_size + i\n",
    "                x_train[i, 0:len(x_list[li]), :] = x_list[li]\n",
    "                y_train[i] = y_list[li]\n",
    "            yield x_train, y_train\n",
    "\n",
    "def val_generator(df,batch_size_val=batches[1][0],batches_per_epoch_val=batches[1][1]):\n",
    "    \n",
    "    num_sequences_val = len(df['emb'].to_list())\n",
    "    assert batch_size_val * batches_per_epoch_val == num_sequences_val\n",
    "    num_features= 768\n",
    "\n",
    "\n",
    "    x_list= df['emb'].to_list()\n",
    "    y_list =  df.label.to_list()\n",
    "    # Generate batches\n",
    "    while True:\n",
    "        for b in range(batches_per_epoch_val):\n",
    "            longest_index = (b + 1) * batch_size_val - 1\n",
    "            timesteps = len(max(df['emb'].to_list()[:(b + 1) * batch_size_val][-31:], key=len))\n",
    "            x_val = np.full((batch_size_val, timesteps, num_features), -99.)\n",
    "            y_val = np.zeros((batch_size_val,  1))\n",
    "            for i in range(batch_size_val):\n",
    "                li = b * batch_size_val + i\n",
    "                x_val[i, 0:len(x_list[li]), :] = x_list[li]\n",
    "                y_val[i] = y_list[li]\n",
    "            yield x_val, y_val\n",
    "\n",
    "def test_generator(df,batch_size_test=batches[2][0],batches_per_epoch_test=batches[2][1]):\n",
    "    \n",
    "    num_sequences_test = len(df['emb'].to_list())\n",
    "    assert batch_size_test * batches_per_epoch_test == num_sequences_test\n",
    "    num_features= 768\n",
    "\n",
    "\n",
    "    x_list= df['emb'].to_list()\n",
    "    y_list =  df.label.to_list()\n",
    "    # Generate batches\n",
    "    while True:\n",
    "        for b in range(batches_per_epoch_test):\n",
    "            longest_index = (b + 1) * batch_size_test - 1\n",
    "            timesteps = len(max(df['emb'].to_list()[:(b + 1) * batch_size_test][-31:], key=len))\n",
    "            # print(len(df_train['emb'].to_list()[:b+batch_size][-7:]))\n",
    "            x_test = np.full((batch_size_test, timesteps, num_features), -99.)\n",
    "            y_test = np.zeros((batch_size_test,  1))\n",
    "            for i in range(batch_size_test):\n",
    "                li = b * batch_size_test + i\n",
    "                x_test[i, 0:len(x_list[li]), :] = x_list[li]\n",
    "                y_test[i] = y_list[li]\n",
    "            yield x_test, y_test            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af6ebf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_generator(df_train)\n",
    "val_data = val_generator(df_val)\n",
    "test_data = test_generator(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c6fcbfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "def cul_all_metrics(y_true, y_pred, pos_label=1):\n",
    "    return {\"accuracy\": float(\"%.5f\" % accuracy_score(y_true=y_true, y_pred=y_pred)),\n",
    "            \"precision\": float(\"%.5f\" % precision_score(y_true=y_true, y_pred=y_pred, pos_label=pos_label)),\n",
    "            \"recall\": float(\"%.5f\" % recall_score(y_true=y_true, y_pred=y_pred, pos_label=pos_label)),\n",
    "            \"f1-score\": float(\"%.5f\" % f1_score(y_true=y_true, y_pred=y_pred)),\n",
    "           }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fee93622",
   "metadata": {},
   "source": [
    "### Step 6.1 : Train & Save LSTM Model For Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d2b3594",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import h5py\n",
    "\n",
    "text_input = keras.Input(shape=(None,768,), dtype='float32', name='text')\n",
    "\n",
    "# keras.layers.Masking(mask_value=0.0)\n",
    "l_mask = keras.layers.Masking(mask_value=-99.)(text_input) \n",
    "\n",
    "# Which we encoded in a single vector via a LSTM\n",
    "encoded_text = keras.layers.LSTM(100,)(l_mask)\n",
    "out_dense = keras.layers.Dense(30, activation='relu')(encoded_text)\n",
    "# And we add a softmax classifier on top\n",
    "out = keras.layers.Dense(2, activation='softmax')(out_dense)\n",
    "# At model instantiation, we specify the input and the output:\n",
    "model = keras.Model(text_input, out)\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['acc'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c3ae957",
   "metadata": {},
   "outputs": [],
   "source": [
    "call_reduce = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_acc', factor=0.95, patience=3, verbose=2,\n",
    "                                mode='auto', min_delta=0.01, cooldown=0, min_lr=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "888919aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "batches_per_epoch = batches[0][1]\n",
    "\n",
    "batches_per_epoch_val= batches[1][1]\n",
    "\n",
    "model.fit(train_data, steps_per_epoch=batches_per_epoch, epochs=10,\n",
    "                    validation_data=val_data, validation_steps=batches_per_epoch_val, callbacks =[call_reduce] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "716252c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_path = \"./trained_models/classification_models_\" + model_path + \"/LSTM_model/model.h5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec162db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save(save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b45d5e97",
   "metadata": {},
   "source": [
    "### Step 6.2 : Evaluate LSTM Model For Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be396c6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# model = tf.keras.models.load_model(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82954709",
   "metadata": {},
   "outputs": [],
   "source": [
    "batches_per_epoch_test = batches[2][1]\n",
    "pred = model.predict_generator(test_data, steps=batches_per_epoch_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c482027e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = np.argmax(pred,axis=1).tolist()\n",
    "label = df_test.label.to_list()\n",
    "\n",
    "cul_all_metrics(label,pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8528db35",
   "metadata": {},
   "source": [
    "### Step 7.1 : Train & Save Transformer Model For Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ed39cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75094044",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadSelfAttention(keras.layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads=8):\n",
    "        super(MultiHeadSelfAttention, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        assert (\n",
    "            embed_dim % num_heads == 0\n",
    "        ), \"embedding dimension not divisible by num heads\"\n",
    "        self.projection_dim = embed_dim // num_heads\n",
    "        self.wq = keras.layers.Dense(embed_dim)\n",
    "        self.wk = keras.layers.Dense(embed_dim)\n",
    "        self.wv = keras.layers.Dense(embed_dim)\n",
    "        self.combine_heads = keras.layers.Dense(embed_dim)\n",
    "\n",
    "    def attention(self, q, k, v):\n",
    "        score = tf.matmul(q, k, transpose_b=True)\n",
    "        dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
    "        scaled_score = score / tf.math.sqrt(dk)\n",
    "        weights = tf.nn.softmax(scaled_score, axis=-1)\n",
    "        output = tf.matmul(weights, v)\n",
    "        return output, weights\n",
    "\n",
    "    def separate_heads(self, x, batch_size):\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.projection_dim))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "\n",
    "    def call(self, x):\n",
    "        # x.shape = [batch_size, seq_len, embedding_dim]\n",
    "        batch_size = tf.shape(x)[0]\n",
    "        q = self.wq(x)  # (batch_size, seq_len, embed_dim)\n",
    "        k = self.wk(x)  # (batch_size, seq_len, embed_dim)\n",
    "        v = self.wv(x)  # (batch_size, seq_len, embed_dim)\n",
    "        q = self.separate_heads(\n",
    "            q, batch_size\n",
    "        )  # (batch_size, num_heads, seq_len, projection_dim)\n",
    "        k = self.separate_heads(\n",
    "            k, batch_size\n",
    "        )  # (batch_size, num_heads, seq_len, projection_dim)\n",
    "        v = self.separate_heads(\n",
    "            v, batch_size\n",
    "        )  # (batch_size, num_heads, seq_len, projection_dim)\n",
    "        attention, weights = self.attention(q, k, v)\n",
    "        attention = tf.transpose(\n",
    "            attention, perm=[0, 2, 1, 3]\n",
    "        )  # (batch_size, seq_len, num_heads, projection_dim)\n",
    "        concat_attention = tf.reshape(\n",
    "            attention, (batch_size, -1, self.embed_dim)\n",
    "        )  # (batch_size, seq_len, embed_dim)\n",
    "        output = self.combine_heads(\n",
    "            concat_attention\n",
    "        )  # (batch_size, seq_len, embed_dim)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd3c18f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super(TransformerLayer, self).__init__()\n",
    "\n",
    "        self.att = MultiHeadSelfAttention(embed_dim, num_heads)\n",
    "        self.ffn = keras.Sequential(\n",
    "            [\n",
    "                keras.layers.Dense(ff_dim, activation=\"relu\"),\n",
    "                keras.layers.Dense(embed_dim),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.layernorm1 = keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "    def call(self, x, training):\n",
    "        attn_output = self.att(x)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(x + attn_output)\n",
    "\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        out2 = self.layernorm2(out1 + ffn_output)\n",
    "\n",
    "        return out2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b8632e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim=768\n",
    "ff_dim=32\n",
    "num_heads=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d54eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_input = keras.Input(shape=(None,768,), dtype='float32', name='text')\n",
    "\n",
    "l_mask = keras.layers.Masking(mask_value=-99.)(text_input) \n",
    "\n",
    "encoded_text = TransformerLayer(embed_dim,num_heads,ff_dim)(l_mask)\n",
    "\n",
    "out_dense1 = keras.layers.LSTM(100,)(encoded_text)\n",
    "\n",
    "out_dense = keras.layers.Dense(30, activation='relu')(out_dense1)\n",
    "\n",
    "out = keras.layers.Dense(2, activation='softmax')(out_dense)\n",
    "\n",
    "model = keras.Model(text_input, out)\n",
    "\n",
    "model.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['acc'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f6761bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "call_reduce = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_acc', factor=0.95, patience=3, verbose=2,\n",
    "                                mode='auto', min_delta=0.01, cooldown=0, min_lr=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9fe100a",
   "metadata": {},
   "outputs": [],
   "source": [
    "batches_per_epoch = batches[0][1]\n",
    "\n",
    "batches_per_epoch_val= batches[1][1]\n",
    "\n",
    "model.fit(train_data, steps_per_epoch=batches_per_epoch, epochs=10,\n",
    "                    validation_data=val_data, validation_steps=batches_per_epoch_val, callbacks =[call_reduce] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96fb0688",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_weight_path = \"./trained_models/classification_models_\" + model_path + \"/Transformer_model/model.h5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a45dddcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save_weights(save_weight_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5caf0c48",
   "metadata": {},
   "source": [
    "### Step 7.2 : Evaluate Transformer Model for Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e398c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = test_generator(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7598acdc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# model.load_weights(save_weight_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42935439",
   "metadata": {},
   "outputs": [],
   "source": [
    "batches_per_epoch_test = batches[2][1]\n",
    "\n",
    "pred = model.predict_generator(test_data, steps=batches_per_epoch_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a9f36d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = np.argmax(pred,axis=1).tolist()\n",
    "label = df_test.label.to_list()\n",
    "\n",
    "cul_all_metrics(label,pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eae6935",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
