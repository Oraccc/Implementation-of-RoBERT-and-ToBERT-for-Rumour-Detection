{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ec3077e",
   "metadata": {},
   "source": [
    "### Step 1.1 : Data Viewing and Simple Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "42f01a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers.data.processors.utils import InputExample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5fc778bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_comments</th>\n",
       "      <th>text_only</th>\n",
       "      <th>comments_only</th>\n",
       "      <th>label</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Breaking: At least 10 dead, 5 injured after tO...</td>\n",
       "      <td>Breaking: At least 10 dead, 5 injured after tO...</td>\n",
       "      <td>The religion of peace strikes again.\\n[SEP]Hi ...</td>\n",
       "      <td>rumour</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>France: 10 people dead after shooting at HQ of...</td>\n",
       "      <td>France: 10 people dead after shooting at HQ of...</td>\n",
       "      <td>MT France: 10 dead after shooting at HQ of sat...</td>\n",
       "      <td>rumour</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ten killed in shooting at headquarters of Fren...</td>\n",
       "      <td>Ten killed in shooting at headquarters of Fren...</td>\n",
       "      <td>must be that peace loving religion again\\n[SEP...</td>\n",
       "      <td>rumour</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BREAKING: 10 dead in shooting at headquarters ...</td>\n",
       "      <td>BREAKING: 10 dead in shooting at headquarters ...</td>\n",
       "      <td>WTF &amp;gt; BREAKING 10 dead in shooting at headq...</td>\n",
       "      <td>rumour</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Reuters: 10 people shot dead at headquarters o...</td>\n",
       "      <td>Reuters: 10 people shot dead at headquarters o...</td>\n",
       "      <td>watch yourself in Paris bud\\n[SEP]islamist ter...</td>\n",
       "      <td>rumour</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       text_comments  \\\n",
       "0  Breaking: At least 10 dead, 5 injured after tO...   \n",
       "1  France: 10 people dead after shooting at HQ of...   \n",
       "2  Ten killed in shooting at headquarters of Fren...   \n",
       "3  BREAKING: 10 dead in shooting at headquarters ...   \n",
       "4  Reuters: 10 people shot dead at headquarters o...   \n",
       "\n",
       "                                           text_only  \\\n",
       "0  Breaking: At least 10 dead, 5 injured after tO...   \n",
       "1  France: 10 people dead after shooting at HQ of...   \n",
       "2  Ten killed in shooting at headquarters of Fren...   \n",
       "3  BREAKING: 10 dead in shooting at headquarters ...   \n",
       "4  Reuters: 10 people shot dead at headquarters o...   \n",
       "\n",
       "                                       comments_only   label  count  \n",
       "0  The religion of peace strikes again.\\n[SEP]Hi ...  rumour      9  \n",
       "1  MT France: 10 dead after shooting at HQ of sat...  rumour      7  \n",
       "2  must be that peace loving religion again\\n[SEP...  rumour      5  \n",
       "3  WTF &gt; BREAKING 10 dead in shooting at headq...  rumour     13  \n",
       "4  watch yourself in Paris bud\\n[SEP]islamist ter...  rumour     16  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data = pd.read_csv('./data/raw_data.csv')\n",
    "raw_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f731331f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "17.496507352941176\n"
     ]
    }
   ],
   "source": [
    "# These codes are used for data statistics only. No need to uncomment.\n",
    "# data = raw_data[raw_data['count'] > 0]\n",
    "# print(data['count'].min())\n",
    "# print(data['count'].mean())\n",
    "\n",
    "# raw_data['len_text'] =raw_data.text_comments.apply(lambda x: len(x.split()))\n",
    "\n",
    "# print(raw_data['len_text'].median())\n",
    "# bins = [0,50,100,150,200,250,300,350,400,450,500]\n",
    "# groups = pd.cut(raw_data['len_text'],bins,right=True)\n",
    "# pd.value_counts(groups).sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "86f39d13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_comments</th>\n",
       "      <th>text_only</th>\n",
       "      <th>comments_only</th>\n",
       "      <th>label</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2484</th>\n",
       "      <td>The black &amp;amp; unarmed group on the left is c...</td>\n",
       "      <td>The black &amp;amp; unarmed group on the left is c...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nonrumour</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2249</th>\n",
       "      <td>Report: #Ferguson police beat up wrong suspect...</td>\n",
       "      <td>Report: #Ferguson police beat up wrong suspect...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>rumour</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2335</th>\n",
       "      <td>People got even angrier after the admission th...</td>\n",
       "      <td>People got even angrier after the admission th...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>rumour</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2204</th>\n",
       "      <td>DARREN WILSON\\nMURDERED AN UNARMED TEEN\\nLEFT ...</td>\n",
       "      <td>DARREN WILSON\\nMURDERED AN UNARMED TEEN\\nLEFT ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>rumour</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2346</th>\n",
       "      <td>American Justice?! #MikeBrown #FergusonShootin...</td>\n",
       "      <td>American Justice?! #MikeBrown #FergusonShootin...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>rumour</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3939</th>\n",
       "      <td>CANADA PARLIAMENT SHOOTING:\\n- Active shooter\\...</td>\n",
       "      <td>CANADA PARLIAMENT SHOOTING:\\n- Active shooter\\...</td>\n",
       "      <td>my beautiful canada!!! No one fucks with canad...</td>\n",
       "      <td>rumour</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4274</th>\n",
       "      <td>You are not alone today #Ottawa - we are here ...</td>\n",
       "      <td>You are not alone today #Ottawa - we are here ...</td>\n",
       "      <td>interestingly - I'm more fearful today here th...</td>\n",
       "      <td>nonrumour</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>700</th>\n",
       "      <td>The right to offend and satirise is  essential...</td>\n",
       "      <td>The right to offend and satirise is  essential...</td>\n",
       "      <td>Tell that to a government who likes taking awa...</td>\n",
       "      <td>nonrumour</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4205</th>\n",
       "      <td>Asked police if there is still gunman on loose...</td>\n",
       "      <td>Asked police if there is still gunman on loose...</td>\n",
       "      <td>“@l_stone: Asked police if there is still gunm...</td>\n",
       "      <td>nonrumour</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>608</th>\n",
       "      <td>Attack on #CharlieHebdo is \"brazen assault on ...</td>\n",
       "      <td>Attack on #CharlieHebdo is \"brazen assault on ...</td>\n",
       "      <td>.@pressfreedom Previously spineless editors sh...</td>\n",
       "      <td>nonrumour</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          text_comments  \\\n",
       "2484  The black &amp; unarmed group on the left is c...   \n",
       "2249  Report: #Ferguson police beat up wrong suspect...   \n",
       "2335  People got even angrier after the admission th...   \n",
       "2204  DARREN WILSON\\nMURDERED AN UNARMED TEEN\\nLEFT ...   \n",
       "2346  American Justice?! #MikeBrown #FergusonShootin...   \n",
       "...                                                 ...   \n",
       "3939  CANADA PARLIAMENT SHOOTING:\\n- Active shooter\\...   \n",
       "4274  You are not alone today #Ottawa - we are here ...   \n",
       "700   The right to offend and satirise is  essential...   \n",
       "4205  Asked police if there is still gunman on loose...   \n",
       "608   Attack on #CharlieHebdo is \"brazen assault on ...   \n",
       "\n",
       "                                              text_only  \\\n",
       "2484  The black &amp; unarmed group on the left is c...   \n",
       "2249  Report: #Ferguson police beat up wrong suspect...   \n",
       "2335  People got even angrier after the admission th...   \n",
       "2204  DARREN WILSON\\nMURDERED AN UNARMED TEEN\\nLEFT ...   \n",
       "2346  American Justice?! #MikeBrown #FergusonShootin...   \n",
       "...                                                 ...   \n",
       "3939  CANADA PARLIAMENT SHOOTING:\\n- Active shooter\\...   \n",
       "4274  You are not alone today #Ottawa - we are here ...   \n",
       "700   The right to offend and satirise is  essential...   \n",
       "4205  Asked police if there is still gunman on loose...   \n",
       "608   Attack on #CharlieHebdo is \"brazen assault on ...   \n",
       "\n",
       "                                          comments_only      label  count  \n",
       "2484                                                NaN  nonrumour      0  \n",
       "2249                                                NaN     rumour      0  \n",
       "2335                                                NaN     rumour      0  \n",
       "2204                                                NaN     rumour      0  \n",
       "2346                                                NaN     rumour      0  \n",
       "...                                                 ...        ...    ...  \n",
       "3939  my beautiful canada!!! No one fucks with canad...     rumour      1  \n",
       "4274  interestingly - I'm more fearful today here th...  nonrumour      1  \n",
       "700   Tell that to a government who likes taking awa...  nonrumour      1  \n",
       "4205  “@l_stone: Asked police if there is still gunm...  nonrumour      1  \n",
       "608   .@pressfreedom Previously spineless editors sh...  nonrumour      1  \n",
       "\n",
       "[500 rows x 5 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data.sort_values(by='count', inplace=True)\n",
    "raw_data.head(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac565718",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5802"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0358b135",
   "metadata": {},
   "outputs": [],
   "source": [
    "### IMPORTANT ###\n",
    "# You may change 'model_path' to save and load different trained models.\n",
    "# Availiable options: 'text_comments','text_only','commments_only','comments_group1','comments_group2','comments_group3','natural_split','fixed_split'.\n",
    "# Please make sure that your 'model_path' must match the correspongding data and comments.\n",
    "# For more details, please check the 'README.md' file.\n",
    "\n",
    "model_path = 'text_comments'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bc5ff3cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Different Number of Comments ##\n",
    "\n",
    "# Please uncomment the corresponding lines if the 'model_path' is 'comments_groupX'.\n",
    "\n",
    "# print(raw_data['count'].describe(percentiles=[0.33,0.67]))\n",
    "\n",
    "# For'comments_group1'.\n",
    "# raw_data = raw_data[raw_data['count'] <= 7]\n",
    "# raw_data.shape\n",
    "\n",
    "# For'comments_group2'.\n",
    "# raw_data = raw_data[raw_data['count'] > 7]\n",
    "# raw_data = raw_data[raw_data['count'] <= 18]\n",
    "# raw_data.shape\n",
    "\n",
    "# For'comments_group3'.\n",
    "# raw_data = raw_data[raw_data['count'] > 18]\n",
    "# raw_data.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "48190230",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Breaking: At least 10 dead, 5 injured after tO...</td>\n",
       "      <td>rumour</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>France: 10 people dead after shooting at HQ of...</td>\n",
       "      <td>rumour</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ten killed in shooting at headquarters of Fren...</td>\n",
       "      <td>rumour</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BREAKING: 10 dead in shooting at headquarters ...</td>\n",
       "      <td>rumour</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Reuters: 10 people shot dead at headquarters o...</td>\n",
       "      <td>rumour</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text   label\n",
       "0  Breaking: At least 10 dead, 5 injured after tO...  rumour\n",
       "1  France: 10 people dead after shooting at HQ of...  rumour\n",
       "2  Ten killed in shooting at headquarters of Fren...  rumour\n",
       "3  BREAKING: 10 dead in shooting at headquarters ...  rumour\n",
       "4  Reuters: 10 people shot dead at headquarters o...  rumour"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Data Selection ##\n",
    "\n",
    "# You may change 'text_comments' to 'text_only' or 'comments_only' with the corresponding 'model_path' to get more experiment results.\n",
    "\n",
    "raw_data = raw_data[['text_comments','label']]\n",
    "raw_data = raw_data.rename(columns = {'text_comments':'text'})\n",
    "\n",
    "# raw_data = raw_data[['text_only','label']]\n",
    "# raw_data = raw_data.rename(columns = {'text_only':'text'})\n",
    "\n",
    "# raw_data = raw_data[['comments_only','label']]\n",
    "# raw_data = raw_data.rename(columns = {'comments_only':'text'})\n",
    "\n",
    "raw_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "80c168aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5802, 2)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data = raw_data.dropna(axis=0)\n",
    "raw_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c43adc08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Breaking: At least 10 dead, 5 injured after tO...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>France: 10 people dead after shooting at HQ of...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ten killed in shooting at headquarters of Fren...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BREAKING: 10 dead in shooting at headquarters ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Reuters: 10 people shot dead at headquarters o...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  Breaking: At least 10 dead, 5 injured after tO...      1\n",
       "1  France: 10 people dead after shooting at HQ of...      1\n",
       "2  Ten killed in shooting at headquarters of Fren...      1\n",
       "3  BREAKING: 10 dead in shooting at headquarters ...      1\n",
       "4  Reuters: 10 people shot dead at headquarters o...      1"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data['label'] = LabelEncoder().fit_transform(raw_data['label'])\n",
    "raw_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e9e1f495",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5151</th>\n",
       "      <td>Sydney police and gov't officials asking publi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3608</th>\n",
       "      <td>Here's what we know so far about the #Germanwi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1452</th>\n",
       "      <td>People gather in silence to remember Paris sho...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2731</th>\n",
       "      <td>On local tv now, reports shooting victim last ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5701</th>\n",
       "      <td>#SydneySiege UPDATE: Gunman identified as loca...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1322</th>\n",
       "      <td>#JeSuisCharlie NO TWEET between 12h00 and 12h0...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>513</th>\n",
       "      <td>BREAKING: French government raises security al...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2248</th>\n",
       "      <td>#Ferguson Chief confirmed name of officer who ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5315</th>\n",
       "      <td>Australia has a long history of terrorism (the...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5471</th>\n",
       "      <td>Much love to everyone tweeting #illridewithyou...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  label\n",
       "5151  Sydney police and gov't officials asking publi...      0\n",
       "3608  Here's what we know so far about the #Germanwi...      0\n",
       "1452  People gather in silence to remember Paris sho...      0\n",
       "2731  On local tv now, reports shooting victim last ...      0\n",
       "5701  #SydneySiege UPDATE: Gunman identified as loca...      0\n",
       "1322  #JeSuisCharlie NO TWEET between 12h00 and 12h0...      0\n",
       "513   BREAKING: French government raises security al...      0\n",
       "2248  #Ferguson Chief confirmed name of officer who ...      1\n",
       "5315  Australia has a long history of terrorism (the...      0\n",
       "5471  Much love to everyone tweeting #illridewithyou...      0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = raw_data.copy()\n",
    "data = data.reindex(np.random.permutation(data.index))\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f15a36ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, val = train_test_split(data, test_size=0.2, random_state=35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d317016d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Our phone lines are open 24/7 for your use, if...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Here's what we do and don't know about the #Sy...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>My thoughts are with the friends and family of...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A Great Grandmother in #Ferguson came out &amp;amp...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Stay Strong Canada.  #ottawashooting\\n[SEP]что...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>\"To kill someone for making fun of you is a ta...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>The artist behind this iconic #CharlieHebdo pe...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Police begged media for discretion about locat...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Several hostages freed at Jewish supermarket i...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>How the federal government is militarizing pol...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  Our phone lines are open 24/7 for your use, if...      0\n",
       "1  Here's what we do and don't know about the #Sy...      0\n",
       "2  My thoughts are with the friends and family of...      0\n",
       "3  A Great Grandmother in #Ferguson came out &amp...      0\n",
       "4  Stay Strong Canada.  #ottawashooting\\n[SEP]что...      0\n",
       "5  \"To kill someone for making fun of you is a ta...      0\n",
       "6  The artist behind this iconic #CharlieHebdo pe...      1\n",
       "7  Police begged media for discretion about locat...      0\n",
       "8  Several hostages freed at Jewish supermarket i...      1\n",
       "9  How the federal government is militarizing pol...      0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.reset_index(drop=True, inplace=True)\n",
    "val.reset_index(drop=True, inplace=True)\n",
    "\n",
    "train.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b6155b0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4641, 2), (1161, 2))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape,val.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efc3a980",
   "metadata": {},
   "source": [
    "### Step 1.2 : Split the Dataset into Segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "57f8ff90",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import get_split,get_natural_split,get_fixed_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7880649f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>text_split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Our phone lines are open 24/7 for your use, if...</td>\n",
       "      <td>0</td>\n",
       "      <td>[Our phone lines are open 24/7 for your use, i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Here's what we do and don't know about the #Sy...</td>\n",
       "      <td>0</td>\n",
       "      <td>[Here's what we do and don't know about the #S...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>My thoughts are with the friends and family of...</td>\n",
       "      <td>0</td>\n",
       "      <td>[My thoughts are with the friends and family o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A Great Grandmother in #Ferguson came out &amp;amp...</td>\n",
       "      <td>0</td>\n",
       "      <td>[A Great Grandmother in #Ferguson came out &amp;am...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Stay Strong Canada.  #ottawashooting\\n[SEP]что...</td>\n",
       "      <td>0</td>\n",
       "      <td>[Stay Strong Canada. #ottawashooting что случи...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label  \\\n",
       "0  Our phone lines are open 24/7 for your use, if...      0   \n",
       "1  Here's what we do and don't know about the #Sy...      0   \n",
       "2  My thoughts are with the friends and family of...      0   \n",
       "3  A Great Grandmother in #Ferguson came out &amp...      0   \n",
       "4  Stay Strong Canada.  #ottawashooting\\n[SEP]что...      0   \n",
       "\n",
       "                                          text_split  \n",
       "0  [Our phone lines are open 24/7 for your use, i...  \n",
       "1  [Here's what we do and don't know about the #S...  \n",
       "2  [My thoughts are with the friends and family o...  \n",
       "3  [A Great Grandmother in #Ferguson came out &am...  \n",
       "4  [Stay Strong Canada. #ottawashooting что случи...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tmp = train.copy()\n",
    "\n",
    "### IMPORTANT ###\n",
    "# If your 'model_path' is 'natural_split' please use 'get_natural_split' function.\n",
    "# If your 'model_path' is 'fixed_split' please use 'get_fixed_split' function.\n",
    "\n",
    "train_tmp['text_split'] = train['text'].apply(get_split)\n",
    "# train_tmp['text_split'] = train['text'].apply(get_fixed_split)\n",
    "# train_tmp['text_split'] = train['text'].apply(get_natural_split)\n",
    "train = train_tmp\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7bc58a04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>text_split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Fuck you, Islamist nutbags. #JeSuisCharlie\\n[S...</td>\n",
       "      <td>0</td>\n",
       "      <td>[Fuck you, Islamist nutbags. #JeSuisCharlie “@...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>friendly reminder. #sydneysiege\\n[SEP]A very g...</td>\n",
       "      <td>0</td>\n",
       "      <td>[friendly reminder. #sydneysiege A very good p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>JUST AHEAD: Could what happened with #Germanwi...</td>\n",
       "      <td>0</td>\n",
       "      <td>[JUST AHEAD: Could what happened with #Germanw...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Germanwings could face enormous legal liabilit...</td>\n",
       "      <td>0</td>\n",
       "      <td>[Germanwings could face enormous legal liabili...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DEVELOPING: #Canada officials identify dead gu...</td>\n",
       "      <td>1</td>\n",
       "      <td>[DEVELOPING: #Canada officials identify dead g...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label  \\\n",
       "0  Fuck you, Islamist nutbags. #JeSuisCharlie\\n[S...      0   \n",
       "1  friendly reminder. #sydneysiege\\n[SEP]A very g...      0   \n",
       "2  JUST AHEAD: Could what happened with #Germanwi...      0   \n",
       "3  Germanwings could face enormous legal liabilit...      0   \n",
       "4  DEVELOPING: #Canada officials identify dead gu...      1   \n",
       "\n",
       "                                          text_split  \n",
       "0  [Fuck you, Islamist nutbags. #JeSuisCharlie “@...  \n",
       "1  [friendly reminder. #sydneysiege A very good p...  \n",
       "2  [JUST AHEAD: Could what happened with #Germanw...  \n",
       "3  [Germanwings could face enormous legal liabili...  \n",
       "4  [DEVELOPING: #Canada officials identify dead g...  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_tmp = val.copy()\n",
    "\n",
    "### IMPORTANT ###\n",
    "# If your 'model_path' is 'natural_split' please use 'get_natural_split' function.\n",
    "# If your 'model_path' is 'fixed_split' please use 'get_fixed_split' function.\n",
    "\n",
    "val_tmp['text_split'] = val['text'].apply(get_split)\n",
    "# val_tmp['text_split'] = val['text'].apply(get_fixed_split)\n",
    "# val_tmp['text_split'] = val['text'].apply(get_natural_split)\n",
    "val = val_tmp\n",
    "val.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "461b2042",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6780, 6780, 6780)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_l = []  # Segmented Text\n",
    "label_l = []  # Label of Each Text\n",
    "index_l =[]   # The Index of Each Text Before Segmentation\n",
    "for idx,row in train.iterrows():\n",
    "  for l in row['text_split']:\n",
    "    train_l.append(l)\n",
    "    label_l.append(row['label'])\n",
    "    index_l.append(idx)\n",
    "len(train_l), len(label_l), len(index_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2ca1239b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1757, 1757, 1757)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_l = []\n",
    "val_label_l = []\n",
    "val_index_l = []\n",
    "for idx,row in val.iterrows():\n",
    "  for l in row['text_split']:\n",
    "    val_l.append(l)\n",
    "    val_label_l.append(row['label'])\n",
    "    val_index_l.append(idx)\n",
    "len(val_l), len(val_label_l), len(val_index_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b3e06d90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Our phone lines are open 24/7 for your use, if...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Here's what we do and don't know about the #Sy...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>My thoughts are with the friends and family of...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A Great Grandmother in #Ferguson came out &amp;amp...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>#MikeBrown. This isour city. #Respect 😊 “@Pres...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  Our phone lines are open 24/7 for your use, if...      0\n",
       "1  Here's what we do and don't know about the #Sy...      0\n",
       "2  My thoughts are with the friends and family of...      0\n",
       "3  A Great Grandmother in #Ferguson came out &amp...      0\n",
       "4  #MikeBrown. This isour city. #Respect 😊 “@Pres...      0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.DataFrame({'text':train_l, 'label':label_l})\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "08e92fca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Fuck you, Islamist nutbags. #JeSuisCharlie “@W...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>friendly reminder. #sydneysiege A very good po...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>JUST AHEAD: Could what happened with #Germanwi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Germanwings could face enormous legal liabilit...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DEVELOPING: #Canada officials identify dead gu...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  Fuck you, Islamist nutbags. #JeSuisCharlie “@W...      0\n",
       "1  friendly reminder. #sydneysiege A very good po...      0\n",
       "2  JUST AHEAD: Could what happened with #Germanwi...      0\n",
       "3  Germanwings could face enormous legal liabilit...      0\n",
       "4  DEVELOPING: #Canada officials identify dead gu...      1"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_df = pd.DataFrame({'text':val_l, 'label':val_label_l})\n",
    "val_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9b9eb037",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_InputExamples = train_df.apply(lambda x: InputExample(guid=None,text_a = x['text'], text_b = None, label = x['label']), axis = 1)\n",
    "\n",
    "val_InputExamples = val_df.apply(lambda x: InputExample(guid=None, text_a = x['text'], text_b = None, label = x['label']), axis = 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c94ce1",
   "metadata": {},
   "source": [
    "### Step 2 : Define Models For Bert Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6fdadd57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import CrossEntropyLoss, MSELoss\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch.optim as optim\n",
    "from torchtext.data import BucketIterator\n",
    "\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler, TensorDataset\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "from transformers import (\n",
    "    WEIGHTS_NAME,\n",
    "    AdamW,\n",
    "    get_linear_schedule_with_warmup,\n",
    "    BertConfig,\n",
    "    BertModel,\n",
    "    BertPreTrainedModel,\n",
    "    BertTokenizer,\n",
    "    BertweetTokenizer,\n",
    "    AutoModel,\n",
    "    AutoTokenizer\n",
    ")\n",
    "\n",
    "from transformers import glue_convert_examples_to_features as convert_examples_to_features\n",
    "from transformers import glue_output_modes as output_modes\n",
    "from transformers import glue_processors as processors\n",
    "from transformers.data.processors.utils import InputExample, DataProcessor\n",
    "\n",
    "import logging\n",
    "\n",
    "logger=logging.getLogger(__name__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "94f5144d",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_CLASSES={\n",
    "    \"bert\":(BertConfig,BertTokenizer),\n",
    "    \"bertweet\":(BertConfig,BertweetTokenizer)\n",
    "}\n",
    "\n",
    "my_label_list=[0, 1]\n",
    "MAX_SEQ_LENGTH=200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "70daba27",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertForClassification(BertPreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.num_labels = 2\n",
    "\n",
    "        self.bert = BertModel(config)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.classifier = nn.Linear(config.hidden_size, self.num_labels)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        token_type_ids=None,\n",
    "        position_ids=None,\n",
    "        head_mask=None,\n",
    "        inputs_embeds=None,\n",
    "        labels=None,\n",
    "    ):\n",
    "\n",
    "        outputs = self.bert(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "        )\n",
    "\n",
    "        sequence_output, pooled_output=outputs[:2]\n",
    "\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.classifier(pooled_output)\n",
    "\n",
    "        \n",
    "        outputs = (logits, pooled_output, sequence_output,)\n",
    "\n",
    "        if labels is not None:\n",
    "            \n",
    "            if self.num_labels == 1:\n",
    "                loss_fct = MSELoss()\n",
    "                loss = loss_fct(logits.view(-1), labels.view(-1))\n",
    "            else:\n",
    "                loss_fct = CrossEntropyLoss()\n",
    "                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "            outputs = (loss,) + outputs\n",
    "\n",
    "        \n",
    "        return outputs  # loss, logits, pooled_output, sequence_output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b72192",
   "metadata": {},
   "source": [
    "### Step 3.1 : Load Pre-training Models & Prepare Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bb4e239b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load Pre-training Models\n",
    "# args={\"model_name_or_path\": \"bert-base-uncased\",\n",
    "#     \"config_name\": \"bert-base-uncased\",\n",
    "#     \"tokenizer_name\": \"bert-base-uncased\",\n",
    "#       }\n",
    "\n",
    "# config_class, tokenizer_class = MODEL_CLASSES[\"bert\"]\n",
    "# model_class=BertForClassification\n",
    "\n",
    "\n",
    "# config = config_class.from_pretrained(\n",
    "#     args[\"config_name\"],\n",
    "#     finetuning_task=\"\", \n",
    "#     cache_dir=None,\n",
    "# )\n",
    "# tokenizer = tokenizer_class.from_pretrained(\n",
    "#     args[\"tokenizer_name\"],\n",
    "#     do_lower_case=True,\n",
    "#     cache_dir=None,\n",
    "# )\n",
    "# model = model_class.from_pretrained(\n",
    "#     args[\"model_name_or_path\"],\n",
    "#     from_tf=bool(\".ckpt\" in args[\"model_name_or_path\"]),\n",
    "#     config=config,\n",
    "#     cache_dir=None,\n",
    "# )\n",
    "\n",
    "\n",
    "# model.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b19bfa10",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# # Prepare Training Data\n",
    "# train_features = convert_examples_to_features(train_InputExamples,tokenizer, label_list=my_label_list, \n",
    "#                                               output_mode=\"classification\", max_length=MAX_SEQ_LENGTH )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "081134e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_ids = torch.tensor([f.input_ids for f in train_features], dtype=torch.long)\n",
    "# attention_mask = torch.tensor([f.attention_mask for f in train_features], dtype=torch.long)\n",
    "# token_type_ids = torch.tensor([f.token_type_ids for f in train_features], dtype=torch.long)\n",
    "# the_labels = torch.tensor([f.label for f in train_features], dtype=torch.long)\n",
    "\n",
    "\n",
    "# dataset = TensorDataset(input_ids, attention_mask, token_type_ids, the_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89481e18",
   "metadata": {},
   "source": [
    "### Step 3.2 : Train & Save Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d094b419",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define Train Function For Bert Classification\n",
    "\n",
    "# def train(train_dataset,model,tokenizer):\n",
    "#     no_decay=[\"bias\",\"LayerNorm.weight\"]\n",
    "#     optimizer_grouped_parameters=[\n",
    "#         {\n",
    "#             \"params\":[p for n,p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "#             \"weight_decay\":0.0,\n",
    "\n",
    "#         },\n",
    "#         {\n",
    "#             \"params\": [p for n,p in model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "#             \"weight_decay\":0.0\n",
    "#         },\n",
    "#     ]\n",
    "\n",
    "    \n",
    "#     t_total=len(train_dataset)// 5\n",
    "#     optimizer=AdamW(optimizer_grouped_parameters,lr=2e-5,eps=1e-8)\n",
    "    \n",
    "#     scheduler=get_linear_schedule_with_warmup(\n",
    "#         optimizer,num_warmup_steps=0,num_training_steps=t_total\n",
    "#         )\n",
    "    \n",
    "    \n",
    "    \n",
    "#     # *********************\n",
    "#     logger.info(\"*****Running training*****\")\n",
    "#     logger.info(\"  Num examples = %d\", len(train_dataset))\n",
    "#     logger.info(\"  Num Epochs = %d\", 5)\n",
    "\n",
    "\n",
    "#     epochs_trained=0\n",
    "#     global_step=0\n",
    "#     steps_trained_in_current_epoch=0\n",
    "\n",
    "#     tr_loss,logging_loss=0.0,0.0\n",
    "#     model.zero_grad()\n",
    "#     train_iterator=trange(epochs_trained,5,desc=\"Epoch\",disable=False)\n",
    "\n",
    "\n",
    "#     for k in train_iterator: #5 epoch\n",
    "    \n",
    "#         train_sampler=RandomSampler(train_dataset)\n",
    "#         train_dataloader=DataLoader(train_dataset,sampler=train_sampler,batch_size=16)\n",
    "#         epoch_iterator=tqdm(train_dataloader,desc=\"Iteration\",disable=False)\n",
    "\n",
    "#         for step,batch in enumerate(epoch_iterator): \n",
    "#             if steps_trained_in_current_epoch>0:\n",
    "#                 steps_trained_in_current_epoch-=1\n",
    "#                 continue\n",
    "\n",
    "#             model.train()\n",
    "#             batch=tuple(t.to(\"cuda\") for t in batch)\n",
    "            \n",
    "#             inputs={\"input_ids\": batch[0],\"attention_mask\": batch[1],\"token_type_ids\": batch[2], \"labels\": batch[3]}\n",
    "#             outputs = model(**inputs)\n",
    "#             loss=outputs[0]\n",
    " \n",
    "#             loss.backward()\n",
    "\n",
    "#             tr_loss+=loss.item()\n",
    "#             if (step+1)%1==0:\n",
    "#                 torch.nn.utils.clip_grad_norm_(model.parameters(),1.0)\n",
    "\n",
    "#                 optimizer.step()\n",
    "#                 scheduler.step()\n",
    "#                 model.zero_grad()\n",
    "#                 global_step+=1\n",
    "\n",
    "#         logger.info(\"average loss:\" +str(tr_loss/global_step))\n",
    "\n",
    "\n",
    "#     return global_step,tr_loss/global_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "be86380b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Start Training\n",
    "\n",
    "# torch.cuda.empty_cache()\n",
    "# train(dataset,model,tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b1ed6847",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save Trained Model Parameters\n",
    "\n",
    "# import os\n",
    "# model.save_pretrained(\"./trained_models/classification_models_\" + model_path)\n",
    "# tokenizer.save_pretrained(\"./trained_models/classification_models_\" + model_path)\n",
    "\n",
    "# torch.save(args,os.path.join(\"./trained_models/classification_models_\" + model_path,\"training_args.bin\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b3204b4",
   "metadata": {},
   "source": [
    "### Step 4.1 : Load the Trained Model & Prepare Data for Bert Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "10539792",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Start Loading the trained model data\n",
    "\n",
    "args_eval={\"model_name_or_path\": \"./trained_models/classification_models_\" + model_path,\n",
    "    \"config_name\": \"./trained_models/classification_models_\" + model_path,\n",
    "    \"tokenizer_name\": \"./trained_models/classification_models_\" + model_path,\n",
    "      }\n",
    "\n",
    "config_class, tokenizer_class = MODEL_CLASSES[\"bert\"]\n",
    "model_class=BertForClassification\n",
    "\n",
    "\n",
    "config = config_class.from_pretrained(\n",
    "    args_eval[\"config_name\"],\n",
    "    finetuning_task=\"\", \n",
    "    cache_dir=None,\n",
    ")\n",
    "tokenizer = tokenizer_class.from_pretrained(\n",
    "    args_eval[\"tokenizer_name\"],\n",
    "    do_lower_case=True,\n",
    "    cache_dir=None,\n",
    ")\n",
    "model = model_class.from_pretrained(\n",
    "    args_eval[\"model_name_or_path\"],\n",
    "    from_tf=bool(\".ckpt\" in args_eval[\"model_name_or_path\"]),\n",
    "    config=config,\n",
    "    cache_dir=None,\n",
    ")\n",
    "\n",
    "\n",
    "model.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a2a5770d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda\\envs\\R_BERT\\lib\\site-packages\\transformers\\data\\processors\\glue.py:67: FutureWarning: This function will be removed from the library soon, preprocessing should be handled with the 🤗 Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/master/examples/pytorch/text-classification/run_glue.py\n",
      "  warnings.warn(DEPRECATION_WARNING.format(\"function\"), FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# Prepare Data for Evaluation\n",
    "\n",
    "val_features = convert_examples_to_features(val_InputExamples, tokenizer, label_list=my_label_list, output_mode=\"classification\",  max_length=MAX_SEQ_LENGTH )\n",
    "\n",
    "\n",
    "val_input_ids = torch.tensor([f.input_ids for f in val_features], dtype=torch.long)\n",
    "val_attention_mask = torch.tensor([f.attention_mask for f in val_features], dtype=torch.long)\n",
    "val_token_type_ids = torch.tensor([f.token_type_ids for f in val_features], dtype=torch.long)\n",
    "val_the_labels = torch.tensor([f.label for f in val_features], dtype=torch.long)\n",
    "\n",
    "\n",
    "eval_dataset = TensorDataset(val_input_ids, val_attention_mask, val_token_type_ids, val_the_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d27663f",
   "metadata": {},
   "source": [
    "### Step 4.2 : Bert Classification Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6e422409",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1a5ba125",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, tokenizer, eval_dataset):\n",
    "\n",
    "\n",
    "    logger.info(\"***** Running evaluation  *****\")\n",
    "    logger.info(\"  Num examples = %d\", len(eval_dataset))\n",
    "    logger.info(\"  Batch size = %d\", 16)\n",
    "    eval_loss = 0.0\n",
    "    nb_eval_steps = 0\n",
    "    preds = None\n",
    "    out_label_ids = None\n",
    "\n",
    "    eval_sampler =RandomSampler(eval_dataset)\n",
    "    eval_dataloader = DataLoader(eval_dataset, sampler=eval_sampler, batch_size=16)\n",
    "\n",
    "    for batch in tqdm(eval_dataloader, desc=\"Evaluating\"):\n",
    "        model.eval()\n",
    "        batch = tuple(t.to(\"cuda\") for t in batch)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            inputs = {\"input_ids\": batch[0], \"attention_mask\": batch[1], \"labels\": batch[3]}\n",
    "            outputs = model(**inputs)\n",
    "            tmp_eval_loss, logits = outputs[:2]\n",
    "\n",
    "            eval_loss += tmp_eval_loss.mean().item()\n",
    "        nb_eval_steps += 1\n",
    "        if preds is None:\n",
    "            preds = logits.detach().cpu().numpy()\n",
    "            out_label_ids = inputs[\"labels\"].detach().cpu().numpy()\n",
    "        else:\n",
    "            preds = np.append(preds, logits.detach().cpu().numpy(), axis=0)\n",
    "            out_label_ids = np.append(out_label_ids, inputs[\"labels\"].detach().cpu().numpy(), axis=0)\n",
    "\n",
    "    eval_loss = eval_loss / nb_eval_steps\n",
    "\n",
    "    preds = np.argmax(preds, axis=1)\n",
    "    \n",
    "    accuracy,f1 = acc_and_f1(preds, out_label_ids)\n",
    "\n",
    "\n",
    "    return accuracy,f1,eval_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e7b2f0e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_accuracy(preds, labels):\n",
    "    return (preds == labels).mean()\n",
    "\n",
    "def acc_and_f1(preds, labels):\n",
    "    acc = simple_accuracy(preds, labels)\n",
    "    f1 = f1_score(y_true=labels, y_pred=preds)\n",
    "    return acc, f1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "37e7256e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 110/110 [00:11<00:00,  9.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.9379624359704041 F1 Score:  0.8995391705069123 Loss:  0.1821067643859847\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "accuracy,f1 ,eval_loss = evaluate(model, tokenizer, eval_dataset)\n",
    "\n",
    "print(\"Accuracy: \",accuracy, \"F1 Score: \",f1,\"Loss: \",eval_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "168cc642",
   "metadata": {},
   "source": [
    "### Step 5.1 : Get Text Embeddings & Combine Embeddings with Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "309a5709",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prediction(model, tokenizer, dataset):\n",
    "\n",
    "    logger.info(\"***** Running prediction  *****\")\n",
    "    logger.info(\"  Num examples = %d\", len(dataset))\n",
    "    logger.info(\"  Batch size = %d\", 16)\n",
    "\n",
    "    pooled_outputs = None\n",
    "\n",
    "    sampler =SequentialSampler(dataset)\n",
    "    dataloader = DataLoader(dataset, sampler=sampler, batch_size=32)\n",
    "\n",
    "    for batch in tqdm(dataloader, desc=\"Evaluating\"):\n",
    "        model.eval()\n",
    "        batch = tuple(t.to(\"cuda\") for t in batch)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            inputs = {\"input_ids\": batch[0], \"attention_mask\": batch[1], \"labels\": batch[3]}\n",
    "            outputs = model(**inputs)\n",
    "            pooled_output = outputs[2]\n",
    "\n",
    "            if pooled_outputs is None:\n",
    "                pooled_outputs = pooled_output.detach().cpu().numpy()\n",
    "            else:\n",
    "                pooled_outputs = np.append(pooled_outputs, pooled_output.detach().cpu().numpy(), axis=0)\n",
    "\n",
    "    return pooled_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "cb0f8389",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args_eval={\"model_name_or_path\": \"./trained_models/classification_models_\" + model_path,\n",
    "    \"config_name\": \"./trained_models/classification_models_\" + model_path,\n",
    "    \"tokenizer_name\": \"./trained_models/classification_models_\" + model_path,\n",
    "      }\n",
    "\n",
    "\n",
    "config_class, tokenizer_class = MODEL_CLASSES[\"bert\"]\n",
    "model_class=BertForClassification\n",
    "\n",
    "\n",
    "config = config_class.from_pretrained(\n",
    "    args_eval[\"config_name\"],\n",
    "    finetuning_task=\"\", \n",
    "    cache_dir=None,\n",
    ")\n",
    "tokenizer = tokenizer_class.from_pretrained(\n",
    "    args_eval[\"tokenizer_name\"],\n",
    "    do_lower_case=True,\n",
    "    cache_dir=None,\n",
    ")\n",
    "model = model_class.from_pretrained(\n",
    "    args_eval[\"model_name_or_path\"],\n",
    "    from_tf=bool(\".ckpt\" in args_eval[\"model_name_or_path\"]),\n",
    "    config=config,\n",
    "    cache_dir=None,\n",
    ")\n",
    "\n",
    "\n",
    "model.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b3b830b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda\\envs\\R_BERT\\lib\\site-packages\\transformers\\data\\processors\\glue.py:67: FutureWarning: This function will be removed from the library soon, preprocessing should be handled with the 🤗 Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/master/examples/pytorch/text-classification/run_glue.py\n",
      "  warnings.warn(DEPRECATION_WARNING.format(\"function\"), FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "train_features = convert_examples_to_features(train_InputExamples,tokenizer, label_list=my_label_list, output_mode=\"classification\", max_length=MAX_SEQ_LENGTH )\n",
    "\n",
    "val_features = convert_examples_to_features(val_InputExamples, tokenizer, label_list=my_label_list, output_mode=\"classification\",  max_length=MAX_SEQ_LENGTH )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "999a86a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input_ids = torch.tensor([f.input_ids for f in train_features], dtype=torch.long)\n",
    "train_attention_mask = torch.tensor([f.attention_mask for f in train_features], dtype=torch.long)\n",
    "train_token_type_ids = torch.tensor([f.token_type_ids for f in train_features], dtype=torch.long)\n",
    "train_the_labels = torch.tensor([f.label for f in train_features], dtype=torch.long)\n",
    "\n",
    "train_dataset = TensorDataset(train_input_ids, train_attention_mask, train_token_type_ids, train_the_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "76e5ee70",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_input_ids = torch.tensor([f.input_ids for f in val_features], dtype=torch.long)\n",
    "val_attention_mask = torch.tensor([f.attention_mask for f in val_features], dtype=torch.long)\n",
    "val_token_type_ids = torch.tensor([f.token_type_ids for f in val_features], dtype=torch.long)\n",
    "val_the_labels = torch.tensor([f.label for f in val_features], dtype=torch.long)\n",
    "\n",
    "val_dataset = TensorDataset(val_input_ids, val_attention_mask, val_token_type_ids, val_the_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "297a56c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 212/212 [00:41<00:00,  5.15it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(6780, 768)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_pooled_outputs = get_prediction(model, tokenizer, train_dataset)\n",
    "train_pooled_outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d4d3df64",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 55/55 [00:10<00:00,  5.20it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1757, 768)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_pooled_outputs = get_prediction(model, tokenizer, val_dataset)\n",
    "val_pooled_outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a729bc29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>emb</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[[0.5908942, -0.48399282, -0.35365856, -0.1273...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[[0.7925307, -0.1366577, -0.07864348, -0.37660...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[[0.17186524, -0.5134308, -0.7181177, 0.679574...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[[0.6241225, -0.30118436, 0.73846936, -0.09697...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[[0.6821581, -0.30926576, -0.19007632, 0.15050...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[[0.8488153, 0.21793163, 0.85684335, -0.369556...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[[0.22909372, -0.14154501, -0.9581366, 0.24813...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[[0.6249268, -0.2554996, -0.45874265, 0.104799...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[[-0.87139964, -0.040248945, 0.8413695, 0.3031...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[[0.77830195, -0.46608713, -0.33662006, -0.157...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 emb  label\n",
       "0  [[0.5908942, -0.48399282, -0.35365856, -0.1273...      0\n",
       "1  [[0.7925307, -0.1366577, -0.07864348, -0.37660...      0\n",
       "2  [[0.17186524, -0.5134308, -0.7181177, 0.679574...      0\n",
       "3  [[0.6241225, -0.30118436, 0.73846936, -0.09697...      0\n",
       "4  [[0.6821581, -0.30926576, -0.19007632, 0.15050...      0\n",
       "5  [[0.8488153, 0.21793163, 0.85684335, -0.369556...      0\n",
       "6  [[0.22909372, -0.14154501, -0.9581366, 0.24813...      1\n",
       "7  [[0.6249268, -0.2554996, -0.45874265, 0.104799...      0\n",
       "8  [[-0.87139964, -0.040248945, 0.8413695, 0.3031...      1\n",
       "9  [[0.77830195, -0.46608713, -0.33662006, -0.157...      0"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Method 1: Feature Concatenation\n",
    "train_x = {}\n",
    "# print(index_l)\n",
    "for l, emb in zip(index_l, train_pooled_outputs):\n",
    "    # print(l)\n",
    "    if l in train_x.keys():\n",
    "        # np.vstack on lists represents features concatenation \n",
    "        train_x[l]  =np.vstack([train_x[l], emb])\n",
    "    else:\n",
    "        train_x[l] = [emb]\n",
    "\n",
    "train_l_final = []\n",
    "label_l_final = []\n",
    "for k in train_x.keys():\n",
    "    train_l_final.append(train_x[k])\n",
    "    label_l_final.append(train.loc[k]['label'])\n",
    "\n",
    "df_train = pd.DataFrame({'emb': train_l_final, 'label': label_l_final})\n",
    "df_train.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c931f6b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>emb</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[[0.64250195, -0.4366386, 0.20196807, 0.047350...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[[0.5000753, -0.42284867, 0.18440875, 0.220372...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[[0.81402516, -0.4456144, -0.123634495, -0.289...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[[0.43503898, -0.04418345, -0.7984998, 0.42404...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[[-0.50168097, 0.28133926, 0.7031337, -0.23010...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[[0.4626501, 0.54636157, -0.13350941, -0.51280...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[[0.6110318, -0.19999708, 0.6508343, -0.013100...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[[0.21398437, -0.46911874, 0.27795646, 0.36079...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[[0.26892766, -0.5820003, 0.32602927, 0.273150...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[[0.59120953, -0.30039522, 0.4800516, -0.01332...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 emb  label\n",
       "0  [[0.64250195, -0.4366386, 0.20196807, 0.047350...      0\n",
       "1  [[0.5000753, -0.42284867, 0.18440875, 0.220372...      0\n",
       "2  [[0.81402516, -0.4456144, -0.123634495, -0.289...      0\n",
       "3  [[0.43503898, -0.04418345, -0.7984998, 0.42404...      0\n",
       "4  [[-0.50168097, 0.28133926, 0.7031337, -0.23010...      1\n",
       "5  [[0.4626501, 0.54636157, -0.13350941, -0.51280...      1\n",
       "6  [[0.6110318, -0.19999708, 0.6508343, -0.013100...      0\n",
       "7  [[0.21398437, -0.46911874, 0.27795646, 0.36079...      0\n",
       "8  [[0.26892766, -0.5820003, 0.32602927, 0.273150...      0\n",
       "9  [[0.59120953, -0.30039522, 0.4800516, -0.01332...      0"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Method 1: Feature Concatenation\n",
    "val_x = {}\n",
    "\n",
    "for l, emb in zip(val_index_l, val_pooled_outputs):\n",
    "    if l in val_x.keys():\n",
    "        val_x[l]  =np.vstack([val_x[l], emb])\n",
    "    else:\n",
    "        val_x[l] = [emb]\n",
    "\n",
    "\n",
    "val_l_final = []\n",
    "vlabel_l_final = []\n",
    "for k in val_x.keys():\n",
    "    val_l_final.append(val_x[k])\n",
    "    vlabel_l_final.append(val.loc[k]['label'])\n",
    "\n",
    "df_val = pd.DataFrame({'emb': val_l_final, 'label': vlabel_l_final})\n",
    "df_val.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6a84d0d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Method 2: Feature Average Pooling\n",
    "# train_x = {}\n",
    "# for l, emb in zip(index_l, train_pooled_outputs):\n",
    "#     if l in train_x.keys():\n",
    "#         train_x[l]  =np.vstack([train_x[l], emb])\n",
    "#     else:\n",
    "#         train_x[l] = [emb]\n",
    "\n",
    "# for l in train_x.keys():\n",
    "#     # print(len(train_x[l]))\n",
    "#     train_x[l] = [np.mean(train_x[l],axis=0)]\n",
    "\n",
    "# train_l_final = []\n",
    "# label_l_final = []\n",
    "# for k in train_x.keys():\n",
    "#     train_l_final.append(train_x[k])\n",
    "#     label_l_final.append(train.loc[k]['label'])\n",
    "\n",
    "# df_train = pd.DataFrame({'emb': train_l_final, 'label': label_l_final})\n",
    "# df_train.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ee847f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Method 2: Feature Average Pooling\n",
    "# val_x = {}\n",
    "\n",
    "# for l, emb in zip(val_index_l, val_pooled_outputs):\n",
    "#     if l in val_x.keys():\n",
    "#         val_x[l]  =np.vstack([val_x[l], emb])\n",
    "#     else:\n",
    "#         val_x[l] = [emb]\n",
    "\n",
    "# for l in val_x.keys():\n",
    "#     val_x[l] = [np.mean(val_x[l],axis=0)]\n",
    "\n",
    "# val_l_final = []\n",
    "# vlabel_l_final = []\n",
    "# for k in val_x.keys():\n",
    "#     val_l_final.append(val_x[k])\n",
    "#     vlabel_l_final.append(val.loc[k]['label'])\n",
    "\n",
    "# df_val = pd.DataFrame({'emb': val_l_final, 'label': vlabel_l_final})\n",
    "# df_val.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "52ad73f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Method 3: Feature Max Pooling\n",
    "# train_x = {}\n",
    "# for l, emb in zip(index_l, train_pooled_outputs):\n",
    "#     if l in train_x.keys():\n",
    "#         train_x[l]  =np.vstack([train_x[l], emb])\n",
    "#     else:\n",
    "#         train_x[l] = [emb]\n",
    "\n",
    "# for l in train_x.keys():\n",
    "#     # print(len(train_x[l]))\n",
    "#     train_x[l] = [np.max(train_x[l],axis=0)]\n",
    "\n",
    "# train_l_final = []\n",
    "# label_l_final = []\n",
    "# for k in train_x.keys():\n",
    "#     train_l_final.append(train_x[k])\n",
    "#     label_l_final.append(train.loc[k]['label'])\n",
    "\n",
    "# df_train = pd.DataFrame({'emb': train_l_final, 'label': label_l_final})\n",
    "# df_train.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6f8fa895",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Method 3: Feature Max Pooling\n",
    "# val_x = {}\n",
    "\n",
    "# for l, emb in zip(val_index_l, val_pooled_outputs):\n",
    "#     if l in val_x.keys():\n",
    "#         val_x[l]  =np.vstack([val_x[l], emb])\n",
    "#     else:\n",
    "#         val_x[l] = [emb]\n",
    "\n",
    "# for l in val_x.keys():\n",
    "#     val_x[l] = [np.max(val_x[l],axis=0)]\n",
    "\n",
    "# val_l_final = []\n",
    "# vlabel_l_final = []\n",
    "# for k in val_x.keys():\n",
    "#     val_l_final.append(val_x[k])\n",
    "#     vlabel_l_final.append(val.loc[k]['label'])\n",
    "\n",
    "# df_val = pd.DataFrame({'emb': val_l_final, 'label': vlabel_l_final})\n",
    "# df_val.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "882b784c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_val, df_test = train_test_split(df_val, test_size=0.4, random_state=35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c4faadf6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4641, 2), (696, 2), (465, 2))"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.shape, df_val.shape, df_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96ff7d01",
   "metadata": {},
   "source": [
    "### Step 5.2 : Prepare Data for Classfication Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "204ebd06",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_dict = {\n",
    "    'text_comments':[[7,663],[3,232],[5,93]],\n",
    "    'text_only':[[7,663],[3,232],[5,93]],\n",
    "    'comments_only':[[4,1088],[4,163],[4,109]],\n",
    "    'comments_group1':[[4,387],[4,58],[5,31]],\n",
    "    'comments_group2':[[4,398],[1,239],[4,40]],\n",
    "    'comments_group3':[[5,300],[5,45],[1,151]],\n",
    "    'natural_split':[[7,663],[3,232],[5,93]],\n",
    "    'fixed_split':[[7,663],[3,232],[5,93]],\n",
    "}\n",
    "\n",
    "batches = batch_dict[model_path]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "5a8941fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_generator(df, batch_size = batches[0][0], batches_per_epoch = batches[0][1]):\n",
    "    num_sequences = len(df['emb'].to_list())\n",
    "    assert batch_size * batches_per_epoch == num_sequences\n",
    "    num_features= 768\n",
    "\n",
    "    x_list= df['emb'].to_list()\n",
    "    y_list =  df.label.to_list()\n",
    "    # Generate batches\n",
    "    while True:\n",
    "        for b in range(batches_per_epoch):\n",
    "            longest_index = (b + 1) * batch_size - 1\n",
    "            timesteps = len(max(df['emb'].to_list()[:(b + 1) * batch_size][-batch_size:], key=len))\n",
    "            x_train = np.full((batch_size, timesteps, num_features), -99.)\n",
    "            y_train = np.zeros((batch_size,  1))\n",
    "            for i in range(batch_size):\n",
    "                li = b * batch_size + i\n",
    "                x_train[i, 0:len(x_list[li]), :] = x_list[li]\n",
    "                y_train[i] = y_list[li]\n",
    "            yield x_train, y_train\n",
    "\n",
    "def val_generator(df,batch_size_val=batches[1][0],batches_per_epoch_val=batches[1][1]):\n",
    "    \n",
    "    num_sequences_val = len(df['emb'].to_list())\n",
    "    assert batch_size_val * batches_per_epoch_val == num_sequences_val\n",
    "    num_features= 768\n",
    "\n",
    "\n",
    "    x_list= df['emb'].to_list()\n",
    "    y_list =  df.label.to_list()\n",
    "    # Generate batches\n",
    "    while True:\n",
    "        for b in range(batches_per_epoch_val):\n",
    "            longest_index = (b + 1) * batch_size_val - 1\n",
    "            timesteps = len(max(df['emb'].to_list()[:(b + 1) * batch_size_val][-31:], key=len))\n",
    "            x_val = np.full((batch_size_val, timesteps, num_features), -99.)\n",
    "            y_val = np.zeros((batch_size_val,  1))\n",
    "            for i in range(batch_size_val):\n",
    "                li = b * batch_size_val + i\n",
    "                x_val[i, 0:len(x_list[li]), :] = x_list[li]\n",
    "                y_val[i] = y_list[li]\n",
    "            yield x_val, y_val\n",
    "\n",
    "def test_generator(df,batch_size_test=batches[2][0],batches_per_epoch_test=batches[2][1]):\n",
    "    \n",
    "    num_sequences_test = len(df['emb'].to_list())\n",
    "    assert batch_size_test * batches_per_epoch_test == num_sequences_test\n",
    "    num_features= 768\n",
    "\n",
    "\n",
    "    x_list= df['emb'].to_list()\n",
    "    y_list =  df.label.to_list()\n",
    "    # Generate batches\n",
    "    while True:\n",
    "        for b in range(batches_per_epoch_test):\n",
    "            longest_index = (b + 1) * batch_size_test - 1\n",
    "            timesteps = len(max(df['emb'].to_list()[:(b + 1) * batch_size_test][-31:], key=len))\n",
    "            # print(len(df_train['emb'].to_list()[:b+batch_size][-7:]))\n",
    "            x_test = np.full((batch_size_test, timesteps, num_features), -99.)\n",
    "            y_test = np.zeros((batch_size_test,  1))\n",
    "            for i in range(batch_size_test):\n",
    "                li = b * batch_size_test + i\n",
    "                x_test[i, 0:len(x_list[li]), :] = x_list[li]\n",
    "                y_test[i] = y_list[li]\n",
    "            yield x_test, y_test            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "1af6ebf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_generator(df_train)\n",
    "val_data = val_generator(df_val)\n",
    "test_data = test_generator(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "1c6fcbfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "def cul_all_metrics(y_true, y_pred, pos_label=1):\n",
    "    return {\"accuracy\": float(\"%.5f\" % accuracy_score(y_true=y_true, y_pred=y_pred)),\n",
    "            \"precision\": float(\"%.5f\" % precision_score(y_true=y_true, y_pred=y_pred, pos_label=pos_label)),\n",
    "            \"recall\": float(\"%.5f\" % recall_score(y_true=y_true, y_pred=y_pred, pos_label=pos_label)),\n",
    "            \"f1-score\": float(\"%.5f\" % f1_score(y_true=y_true, y_pred=y_pred)),\n",
    "           }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fee93622",
   "metadata": {},
   "source": [
    "### Step 6.1 : Train & Save LSTM Model For Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "4d2b3594",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "text (InputLayer)            [(None, None, 768)]       0         \n",
      "_________________________________________________________________\n",
      "masking (Masking)            (None, None, 768)         0         \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 100)               347600    \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 30)                3030      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 62        \n",
      "=================================================================\n",
      "Total params: 350,692\n",
      "Trainable params: 350,692\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import h5py\n",
    "\n",
    "text_input = keras.Input(shape=(None,768,), dtype='float32', name='text')\n",
    "\n",
    "# keras.layers.Masking(mask_value=0.0)\n",
    "l_mask = keras.layers.Masking(mask_value=-99.)(text_input) \n",
    "\n",
    "# Which we encoded in a single vector via a LSTM\n",
    "encoded_text = keras.layers.LSTM(100,)(l_mask)\n",
    "out_dense = keras.layers.Dense(30, activation='relu')(encoded_text)\n",
    "# And we add a softmax classifier on top\n",
    "out = keras.layers.Dense(2, activation='softmax')(out_dense)\n",
    "# At model instantiation, we specify the input and the output:\n",
    "model = keras.Model(text_input, out)\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['acc'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "7c3ae957",
   "metadata": {},
   "outputs": [],
   "source": [
    "call_reduce = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_acc', factor=0.95, patience=3, verbose=2,\n",
    "                                mode='auto', min_delta=0.01, cooldown=0, min_lr=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "888919aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 663 steps, validate for 232 steps\n",
      "Epoch 1/10\n",
      "663/663 [==============================] - 7s 10ms/step - loss: 0.1464 - acc: 0.9537 - val_loss: 0.1453 - val_acc: 0.9511\n",
      "Epoch 2/10\n",
      "663/663 [==============================] - 3s 5ms/step - loss: 0.1284 - acc: 0.9580 - val_loss: 0.1442 - val_acc: 0.9497\n",
      "Epoch 3/10\n",
      "663/663 [==============================] - 3s 5ms/step - loss: 0.1209 - acc: 0.9595 - val_loss: 0.1434 - val_acc: 0.9569\n",
      "Epoch 4/10\n",
      "649/663 [============================>.] - ETA: 0s - loss: 0.1148 - acc: 0.9619\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.0009500000451225787.\n",
      "663/663 [==============================] - 3s 5ms/step - loss: 0.1145 - acc: 0.9621 - val_loss: 0.1446 - val_acc: 0.9598\n",
      "Epoch 5/10\n",
      "663/663 [==============================] - 3s 5ms/step - loss: 0.1097 - acc: 0.9634 - val_loss: 0.1352 - val_acc: 0.9612\n",
      "Epoch 6/10\n",
      "663/663 [==============================] - 3s 5ms/step - loss: 0.1026 - acc: 0.9651 - val_loss: 0.1430 - val_acc: 0.9569\n",
      "Epoch 7/10\n",
      "663/663 [==============================] - 3s 5ms/step - loss: 0.0982 - acc: 0.9672 - val_loss: 0.1495 - val_acc: 0.9540\n",
      "Epoch 8/10\n",
      "657/663 [============================>.] - ETA: 0s - loss: 0.0944 - acc: 0.9693\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.0009025000152178108.\n",
      "663/663 [==============================] - 3s 5ms/step - loss: 0.0937 - acc: 0.9696 - val_loss: 0.1519 - val_acc: 0.9511\n",
      "Epoch 9/10\n",
      "663/663 [==============================] - 3s 5ms/step - loss: 0.0885 - acc: 0.9696 - val_loss: 0.1509 - val_acc: 0.9526\n",
      "Epoch 10/10\n",
      "663/663 [==============================] - 3s 5ms/step - loss: 0.0833 - acc: 0.9722 - val_loss: 0.1537 - val_acc: 0.9511\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x21a59433048>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batches_per_epoch = batches[0][1]\n",
    "\n",
    "batches_per_epoch_val= batches[1][1]\n",
    "\n",
    "model.fit(train_data, steps_per_epoch=batches_per_epoch, epochs=10,\n",
    "                    validation_data=val_data, validation_steps=batches_per_epoch_val, callbacks =[call_reduce] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "716252c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_path = \"./trained_models/classification_models_\" + model_path + \"/LSTM_model/model.h5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "eec162db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save(save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b45d5e97",
   "metadata": {},
   "source": [
    "### Step 6.2 : Evaluate LSTM Model For Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "4be396c6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# model = tf.keras.models.load_model(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "82954709",
   "metadata": {},
   "outputs": [],
   "source": [
    "batches_per_epoch_test = batches[2][1]\n",
    "pred = model.predict_generator(test_data, steps=batches_per_epoch_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "c482027e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.96129,\n",
       " 'precision': 0.96575,\n",
       " 'recall': 0.91558,\n",
       " 'f1-score': 0.94}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = np.argmax(pred,axis=1).tolist()\n",
    "label = df_test.label.to_list()\n",
    "\n",
    "cul_all_metrics(label,pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8528db35",
   "metadata": {},
   "source": [
    "### Step 7.1 : Train & Save Transformer Model For Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b9ed39cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "75094044",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadSelfAttention(keras.layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads=8):\n",
    "        super(MultiHeadSelfAttention, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        assert (\n",
    "            embed_dim % num_heads == 0\n",
    "        ), \"embedding dimension not divisible by num heads\"\n",
    "        self.projection_dim = embed_dim // num_heads\n",
    "        self.wq = keras.layers.Dense(embed_dim)\n",
    "        self.wk = keras.layers.Dense(embed_dim)\n",
    "        self.wv = keras.layers.Dense(embed_dim)\n",
    "        self.combine_heads = keras.layers.Dense(embed_dim)\n",
    "\n",
    "    def attention(self, q, k, v):\n",
    "        score = tf.matmul(q, k, transpose_b=True)\n",
    "        dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
    "        scaled_score = score / tf.math.sqrt(dk)\n",
    "        weights = tf.nn.softmax(scaled_score, axis=-1)\n",
    "        output = tf.matmul(weights, v)\n",
    "        return output, weights\n",
    "\n",
    "    def separate_heads(self, x, batch_size):\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.projection_dim))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "\n",
    "    def call(self, x):\n",
    "        # x.shape = [batch_size, seq_len, embedding_dim]\n",
    "        batch_size = tf.shape(x)[0]\n",
    "        q = self.wq(x)  # (batch_size, seq_len, embed_dim)\n",
    "        k = self.wk(x)  # (batch_size, seq_len, embed_dim)\n",
    "        v = self.wv(x)  # (batch_size, seq_len, embed_dim)\n",
    "        q = self.separate_heads(\n",
    "            q, batch_size\n",
    "        )  # (batch_size, num_heads, seq_len, projection_dim)\n",
    "        k = self.separate_heads(\n",
    "            k, batch_size\n",
    "        )  # (batch_size, num_heads, seq_len, projection_dim)\n",
    "        v = self.separate_heads(\n",
    "            v, batch_size\n",
    "        )  # (batch_size, num_heads, seq_len, projection_dim)\n",
    "        attention, weights = self.attention(q, k, v)\n",
    "        attention = tf.transpose(\n",
    "            attention, perm=[0, 2, 1, 3]\n",
    "        )  # (batch_size, seq_len, num_heads, projection_dim)\n",
    "        concat_attention = tf.reshape(\n",
    "            attention, (batch_size, -1, self.embed_dim)\n",
    "        )  # (batch_size, seq_len, embed_dim)\n",
    "        output = self.combine_heads(\n",
    "            concat_attention\n",
    "        )  # (batch_size, seq_len, embed_dim)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "0bd3c18f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super(TransformerLayer, self).__init__()\n",
    "\n",
    "        self.att = MultiHeadSelfAttention(embed_dim, num_heads)\n",
    "        self.ffn = keras.Sequential(\n",
    "            [\n",
    "                keras.layers.Dense(ff_dim, activation=\"relu\"),\n",
    "                keras.layers.Dense(embed_dim),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.layernorm1 = keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "    def call(self, x, training):\n",
    "        attn_output = self.att(x)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(x + attn_output)\n",
    "\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        out2 = self.layernorm2(out1 + ffn_output)\n",
    "\n",
    "        return out2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "0b8632e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim=768\n",
    "ff_dim=32\n",
    "num_heads=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "67d54eba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "text (InputLayer)            [(None, None, 768)]       0         \n",
      "_________________________________________________________________\n",
      "masking_1 (Masking)          (None, None, 768)         0         \n",
      "_________________________________________________________________\n",
      "transformer_layer (Transform (None, None, 768)         2415392   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 100)               347600    \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 30)                3030      \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 2)                 62        \n",
      "=================================================================\n",
      "Total params: 2,766,084\n",
      "Trainable params: 2,766,084\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "text_input = keras.Input(shape=(None,768,), dtype='float32', name='text')\n",
    "\n",
    "l_mask = keras.layers.Masking(mask_value=-99.)(text_input) \n",
    "\n",
    "encoded_text = TransformerLayer(embed_dim,num_heads,ff_dim)(l_mask)\n",
    "\n",
    "out_dense1 = keras.layers.LSTM(100,)(encoded_text)\n",
    "\n",
    "out_dense = keras.layers.Dense(30, activation='relu')(out_dense1)\n",
    "\n",
    "out = keras.layers.Dense(2, activation='softmax')(out_dense)\n",
    "\n",
    "model = keras.Model(text_input, out)\n",
    "\n",
    "model.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['acc'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "9f6761bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "call_reduce = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_acc', factor=0.95, patience=3, verbose=2,\n",
    "                                mode='auto', min_delta=0.01, cooldown=0, min_lr=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "b9fe100a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 663 steps, validate for 232 steps\n",
      "Epoch 1/10\n",
      "663/663 [==============================] - 12s 17ms/step - loss: 0.1709 - acc: 0.9504 - val_loss: 0.1501 - val_acc: 0.9540\n",
      "Epoch 2/10\n",
      "663/663 [==============================] - 9s 14ms/step - loss: 0.1642 - acc: 0.9487 - val_loss: 0.1813 - val_acc: 0.9440\n",
      "Epoch 3/10\n",
      "663/663 [==============================] - 9s 14ms/step - loss: 0.1854 - acc: 0.9453 - val_loss: 0.1936 - val_acc: 0.9497\n",
      "Epoch 4/10\n",
      "659/663 [============================>.] - ETA: 0s - loss: 0.1834 - acc: 0.9460\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.0009500000451225787.\n",
      "663/663 [==============================] - 9s 14ms/step - loss: 0.1832 - acc: 0.9459 - val_loss: 0.1795 - val_acc: 0.9468\n",
      "Epoch 5/10\n",
      "663/663 [==============================] - 9s 14ms/step - loss: 0.1719 - acc: 0.9470 - val_loss: 0.1441 - val_acc: 0.9526\n",
      "Epoch 6/10\n",
      "663/663 [==============================] - 9s 14ms/step - loss: 0.1613 - acc: 0.9502 - val_loss: 0.1598 - val_acc: 0.9511\n",
      "Epoch 7/10\n",
      "659/663 [============================>.] - ETA: 0s - loss: 0.1492 - acc: 0.9540\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.0009025000152178108.\n",
      "663/663 [==============================] - 9s 14ms/step - loss: 0.1485 - acc: 0.9543 - val_loss: 0.1534 - val_acc: 0.9555\n",
      "Epoch 8/10\n",
      "663/663 [==============================] - 9s 14ms/step - loss: 0.1485 - acc: 0.9543 - val_loss: 0.1530 - val_acc: 0.9540\n",
      "Epoch 9/10\n",
      "663/663 [==============================] - 9s 14ms/step - loss: 0.1448 - acc: 0.9563 - val_loss: 0.1428 - val_acc: 0.9555\n",
      "Epoch 10/10\n",
      "662/663 [============================>.] - ETA: 0s - loss: 0.1338 - acc: 0.9571\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.0008573750033974647.\n",
      "663/663 [==============================] - 9s 14ms/step - loss: 0.1336 - acc: 0.9571 - val_loss: 0.1392 - val_acc: 0.9569\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x21ad7c75908>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batches_per_epoch = batches[0][1]\n",
    "\n",
    "batches_per_epoch_val= batches[1][1]\n",
    "\n",
    "model.fit(train_data, steps_per_epoch=batches_per_epoch, epochs=10,\n",
    "                    validation_data=val_data, validation_steps=batches_per_epoch_val, callbacks =[call_reduce] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "96fb0688",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_weight_path = \"./trained_models/classification_models_\" + model_path + \"/Transformer_model/model.h5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "a45dddcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save_weights(save_weight_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5caf0c48",
   "metadata": {},
   "source": [
    "### Step 7.2 : Evaluate Transformer Model for Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "9e398c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = test_generator(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "7598acdc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# model.load_weights(save_weight_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "42935439",
   "metadata": {},
   "outputs": [],
   "source": [
    "batches_per_epoch_test = batches[2][1]\n",
    "\n",
    "pred = model.predict_generator(test_data, steps=batches_per_epoch_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "1a9f36d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.96344,\n",
       " 'precision': 0.94771,\n",
       " 'recall': 0.94156,\n",
       " 'f1-score': 0.94463}"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = np.argmax(pred,axis=1).tolist()\n",
    "label = df_test.label.to_list()\n",
    "\n",
    "cul_all_metrics(label,pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eae6935",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
